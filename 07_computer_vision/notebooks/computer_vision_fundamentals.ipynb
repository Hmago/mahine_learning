{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0aa19df",
   "metadata": {},
   "source": [
    "# Computer Vision Fundamentals - Interactive Learning Notebook üëÅÔ∏èü§ñ\n",
    "\n",
    "Welcome to your hands-on computer vision journey! This notebook provides interactive examples and exercises to master computer vision concepts.\n",
    "\n",
    "## üéØ What You'll Learn\n",
    "\n",
    "1. **Image Processing Basics**: Loading, displaying, and manipulating images\n",
    "2. **Feature Detection**: Finding important patterns in images  \n",
    "3. **Object Detection**: Identifying and locating objects\n",
    "4. **Real-world Applications**: Building practical computer vision systems\n",
    "\n",
    "## üõ†Ô∏è Setup\n",
    "\n",
    "First, let's import all necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b383679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run this cell first)\n",
    "!pip install opencv-python matplotlib numpy pillow scikit-image\n",
    "!pip install ultralytics  # For YOLO object detection\n",
    "\n",
    "print(\"‚úÖ Installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376e275d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up matplotlib for better display\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "print(\"üìö Libraries imported successfully!\")\n",
    "print(f\"OpenCV version: {cv2.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73308bcf",
   "metadata": {},
   "source": [
    "## üì∏ Section 1: Image Processing Fundamentals\n",
    "\n",
    "Let's start with the basics - how computers \"see\" and process images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d384fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load a sample image from the internet\n",
    "def load_sample_image(url=None):\n",
    "    \"\"\"Load a sample image for experimentation\"\"\"\n",
    "    \n",
    "    if url is None:\n",
    "        # Use a sample image URL (feel free to replace with your own)\n",
    "        url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/5/50/Vd-Orig.png/256px-Vd-Orig.png\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        image = Image.open(BytesIO(response.content))\n",
    "        # Convert to OpenCV format (BGR)\n",
    "        image_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "        return image_cv\n",
    "    except:\n",
    "        # Create a simple test image if download fails\n",
    "        print(\"Creating a test image...\")\n",
    "        test_image = np.zeros((300, 400, 3), dtype=np.uint8)\n",
    "        cv2.rectangle(test_image, (50, 50), (150, 150), (0, 255, 0), -1)\n",
    "        cv2.circle(test_image, (300, 100), 50, (255, 0, 0), -1)\n",
    "        cv2.putText(test_image, 'Test Image', (50, 250), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "        return test_image\n",
    "\n",
    "# Load our sample image\n",
    "sample_image = load_sample_image()\n",
    "print(f\"‚úÖ Image loaded! Shape: {sample_image.shape}\")\n",
    "print(f\"   - Height: {sample_image.shape[0]} pixels\")\n",
    "print(f\"   - Width: {sample_image.shape[1]} pixels\")\n",
    "print(f\"   - Channels: {sample_image.shape[2]} (BGR format)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd56ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the image\n",
    "def display_image(image, title=\"Image\", cmap=None):\n",
    "    \"\"\"Helper function to display images properly\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    if len(image.shape) == 3:\n",
    "        # Color image - convert BGR to RGB for display\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        plt.imshow(image_rgb)\n",
    "    else:\n",
    "        # Grayscale image\n",
    "        plt.imshow(image, cmap='gray')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Display our sample image\n",
    "display_image(sample_image, \"Our Sample Image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdd1602",
   "metadata": {},
   "source": [
    "### üé® Understanding Image Properties\n",
    "\n",
    "Let's explore what makes up a digital image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f21b412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze image properties\n",
    "def analyze_image_properties(image):\n",
    "    \"\"\"Analyze and display various image properties\"\"\"\n",
    "    \n",
    "    print(\"üîç Image Analysis:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Basic properties\n",
    "    print(f\"üìê Dimensions: {image.shape}\")\n",
    "    print(f\"üíæ Data type: {image.dtype}\")\n",
    "    print(f\"üìä Total pixels: {image.size:,}\")\n",
    "    print(f\"üí≠ Memory usage: {image.nbytes / (1024*1024):.2f} MB\")\n",
    "    \n",
    "    # Pixel value statistics\n",
    "    print(f\"\\nüìà Pixel Value Statistics:\")\n",
    "    print(f\"   Minimum value: {image.min()}\")\n",
    "    print(f\"   Maximum value: {image.max()}\")\n",
    "    print(f\"   Mean value: {image.mean():.2f}\")\n",
    "    print(f\"   Standard deviation: {image.std():.2f}\")\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    # Original image\n",
    "    axes[0, 0].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    axes[0, 0].set_title('Original Image')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    # Color channels\n",
    "    b, g, r = cv2.split(image)\n",
    "    \n",
    "    axes[0, 1].imshow(r, cmap='Reds')\n",
    "    axes[0, 1].set_title('Red Channel')\n",
    "    axes[0, 1].axis('off')\n",
    "    \n",
    "    axes[0, 2].imshow(g, cmap='Greens')\n",
    "    axes[0, 2].set_title('Green Channel')\n",
    "    axes[0, 2].axis('off')\n",
    "    \n",
    "    axes[1, 0].imshow(b, cmap='Blues')\n",
    "    axes[1, 0].set_title('Blue Channel')\n",
    "    axes[1, 0].axis('off')\n",
    "    \n",
    "    # Histogram\n",
    "    colors = ['red', 'green', 'blue']\n",
    "    for i, color in enumerate(colors):\n",
    "        hist = cv2.calcHist([image], [i], None, [256], [0, 256])\n",
    "        axes[1, 1].plot(hist, color=color, alpha=0.7)\n",
    "    axes[1, 1].set_title('Color Histogram')\n",
    "    axes[1, 1].set_xlabel('Pixel Value')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    \n",
    "    # Grayscale version\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    axes[1, 2].imshow(gray, cmap='gray')\n",
    "    axes[1, 2].set_title('Grayscale')\n",
    "    axes[1, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Analyze our sample image\n",
    "analyze_image_properties(sample_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a82af1e",
   "metadata": {},
   "source": [
    "### üîß Basic Image Operations\n",
    "\n",
    "Now let's learn to transform and enhance images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e99fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic image transformations\n",
    "def demonstrate_transformations(image):\n",
    "    \"\"\"Demonstrate basic image transformations\"\"\"\n",
    "    \n",
    "    # Resize\n",
    "    resized = cv2.resize(image, (300, 200))\n",
    "    \n",
    "    # Rotate\n",
    "    height, width = image.shape[:2]\n",
    "    rotation_matrix = cv2.getRotationMatrix2D((width/2, height/2), 45, 1)\n",
    "    rotated = cv2.warpAffine(image, rotation_matrix, (width, height))\n",
    "    \n",
    "    # Flip\n",
    "    flipped_horizontal = cv2.flip(image, 1)\n",
    "    flipped_vertical = cv2.flip(image, 0)\n",
    "    \n",
    "    # Brightness adjustment\n",
    "    brighter = cv2.convertScaleAbs(image, alpha=1.0, beta=50)\n",
    "    darker = cv2.convertScaleAbs(image, alpha=1.0, beta=-50)\n",
    "    \n",
    "    # Display results\n",
    "    transformations = [\n",
    "        (image, 'Original'),\n",
    "        (resized, 'Resized'),\n",
    "        (rotated, 'Rotated 45¬∞'),\n",
    "        (flipped_horizontal, 'Flipped Horizontal'),\n",
    "        (brighter, 'Brighter'),\n",
    "        (darker, 'Darker')\n",
    "    ]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, (img, title) in enumerate(transformations):\n",
    "        if len(img.shape) == 3:\n",
    "            img_display = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        else:\n",
    "            img_display = img\n",
    "        \n",
    "        axes[i].imshow(img_display)\n",
    "        axes[i].set_title(title)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Demonstrate transformations\n",
    "demonstrate_transformations(sample_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e3079c",
   "metadata": {},
   "source": [
    "### üéõÔ∏è Image Filtering and Enhancement\n",
    "\n",
    "Learn to improve image quality and extract important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aa4235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image filtering demonstrations\n",
    "def demonstrate_filtering(image):\n",
    "    \"\"\"Demonstrate various image filters\"\"\"\n",
    "    \n",
    "    # Convert to grayscale for some operations\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Gaussian blur\n",
    "    blurred = cv2.GaussianBlur(image, (15, 15), 0)\n",
    "    \n",
    "    # Sharpening\n",
    "    sharpening_kernel = np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]])\n",
    "    sharpened = cv2.filter2D(image, -1, sharpening_kernel)\n",
    "    \n",
    "    # Edge detection\n",
    "    edges_canny = cv2.Canny(gray, 50, 150)\n",
    "    edges_sobel = cv2.Sobel(gray, cv2.CV_64F, 1, 1, ksize=3)\n",
    "    edges_sobel = np.uint8(np.absolute(edges_sobel))\n",
    "    \n",
    "    # Noise reduction\n",
    "    denoised = cv2.medianBlur(image, 5)\n",
    "    \n",
    "    # Display results\n",
    "    filters = [\n",
    "        (image, 'Original', 'color'),\n",
    "        (blurred, 'Gaussian Blur', 'color'),\n",
    "        (sharpened, 'Sharpened', 'color'),\n",
    "        (edges_canny, 'Canny Edges', 'gray'),\n",
    "        (edges_sobel, 'Sobel Edges', 'gray'),\n",
    "        (denoised, 'Denoised', 'color')\n",
    "    ]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, (img, title, img_type) in enumerate(filters):\n",
    "        if img_type == 'color':\n",
    "            img_display = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            axes[i].imshow(img_display)\n",
    "        else:\n",
    "            axes[i].imshow(img, cmap='gray')\n",
    "        \n",
    "        axes[i].set_title(title)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Demonstrate filtering\n",
    "demonstrate_filtering(sample_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac5570b",
   "metadata": {},
   "source": [
    "## üîç Section 2: Feature Detection\n",
    "\n",
    "Now let's find important patterns and landmarks in images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7b2f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corner detection demonstration\n",
    "def detect_corners(image):\n",
    "    \"\"\"Demonstrate corner detection methods\"\"\"\n",
    "    \n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Harris corner detection\n",
    "    harris_corners = cv2.cornerHarris(gray, 2, 3, 0.04)\n",
    "    harris_corners = cv2.dilate(harris_corners, None)\n",
    "    \n",
    "    # Shi-Tomasi corner detection  \n",
    "    shi_tomasi_corners = cv2.goodFeaturesToTrack(gray, maxCorners=100, qualityLevel=0.01, minDistance=10)\n",
    "    \n",
    "    # FAST corner detection\n",
    "    fast = cv2.FastFeatureDetector_create()\n",
    "    fast_keypoints = fast.detect(gray, None)\n",
    "    \n",
    "    # Visualize results\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # Harris corners\n",
    "    harris_img = image.copy()\n",
    "    harris_img[harris_corners > 0.01 * harris_corners.max()] = [0, 0, 255]\n",
    "    axes[0].imshow(cv2.cvtColor(harris_img, cv2.COLOR_BGR2RGB))\n",
    "    axes[0].set_title('Harris Corner Detection')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Shi-Tomasi corners\n",
    "    shi_tomasi_img = image.copy()\n",
    "    if shi_tomasi_corners is not None:\n",
    "        for corner in shi_tomasi_corners:\n",
    "            x, y = corner.ravel()\n",
    "            cv2.circle(shi_tomasi_img, (int(x), int(y)), 3, (0, 255, 0), -1)\n",
    "    axes[1].imshow(cv2.cvtColor(shi_tomasi_img, cv2.COLOR_BGR2RGB))\n",
    "    axes[1].set_title(f'Shi-Tomasi ({len(shi_tomasi_corners) if shi_tomasi_corners is not None else 0} corners)')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # FAST corners\n",
    "    fast_img = cv2.drawKeypoints(image, fast_keypoints, None, color=(255, 0, 0))\n",
    "    axes[2].imshow(cv2.cvtColor(fast_img, cv2.COLOR_BGR2RGB))\n",
    "    axes[2].set_title(f'FAST ({len(fast_keypoints)} keypoints)')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üéØ Corner Detection Results:\")\n",
    "    print(f\"   Harris corners: Found distinct corner regions\")\n",
    "    print(f\"   Shi-Tomasi: {len(shi_tomasi_corners) if shi_tomasi_corners is not None else 0} good corners\")\n",
    "    print(f\"   FAST: {len(fast_keypoints)} fast keypoints\")\n",
    "\n",
    "# Detect corners in our sample image\n",
    "detect_corners(sample_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c679ce",
   "metadata": {},
   "source": [
    "## üéØ Section 3: Object Detection\n",
    "\n",
    "Let's build systems that can identify and locate objects in images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af9b8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object detection with YOLO\n",
    "try:\n",
    "    from ultralytics import YOLO\n",
    "    \n",
    "    def detect_objects_yolo(image):\n",
    "        \"\"\"Detect objects using YOLO\"\"\"\n",
    "        \n",
    "        print(\"üîÑ Loading YOLO model...\")\n",
    "        model = YOLO('yolov8n.pt')  # This will download the model if not present\n",
    "        \n",
    "        print(\"üîç Running object detection...\")\n",
    "        results = model(image, verbose=False)\n",
    "        \n",
    "        # Process results\n",
    "        detections = []\n",
    "        annotated_image = image.copy()\n",
    "        \n",
    "        for r in results:\n",
    "            if r.boxes is not None:\n",
    "                for box in r.boxes:\n",
    "                    # Extract detection info\n",
    "                    x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
    "                    confidence = float(box.conf[0])\n",
    "                    class_id = int(box.cls[0])\n",
    "                    class_name = model.names[class_id]\n",
    "                    \n",
    "                    detections.append({\n",
    "                        'class': class_name,\n",
    "                        'confidence': confidence,\n",
    "                        'bbox': [x1, y1, x2, y2]\n",
    "                    })\n",
    "                    \n",
    "                    # Draw bounding box\n",
    "                    cv2.rectangle(annotated_image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "                    \n",
    "                    # Draw label\n",
    "                    label = f\"{class_name}: {confidence:.2f}\"\n",
    "                    label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]\n",
    "                    cv2.rectangle(annotated_image, (int(x1), int(y1) - label_size[1] - 10), \n",
    "                                 (int(x1) + label_size[0], int(y1)), (0, 255, 0), -1)\n",
    "                    cv2.putText(annotated_image, label, (int(x1), int(y1) - 5), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)\n",
    "        \n",
    "        # Display results\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "        \n",
    "        axes[0].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        axes[0].set_title('Original Image')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        axes[1].imshow(cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB))\n",
    "        axes[1].set_title(f'Object Detection Results ({len(detections)} objects)')\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print detection summary\n",
    "        if detections:\n",
    "            print(\"üéØ Detection Results:\")\n",
    "            for i, det in enumerate(detections, 1):\n",
    "                print(f\"   {i}. {det['class']}: {det['confidence']:.2f} confidence\")\n",
    "        else:\n",
    "            print(\"‚ÑπÔ∏è  No objects detected in this image.\")\n",
    "        \n",
    "        return detections, annotated_image\n",
    "    \n",
    "    # Run object detection\n",
    "    detections, annotated = detect_objects_yolo(sample_image)\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  YOLO not available. Install with: pip install ultralytics\")\n",
    "    print(\"For now, let's demonstrate a simple object detection concept...\")\n",
    "    \n",
    "    def simple_circle_detection(image):\n",
    "        \"\"\"Detect circles as a simple object detection example\"\"\"\n",
    "        \n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Use HoughCircles to detect circular objects\n",
    "        circles = cv2.HoughCircles(\n",
    "            gray, cv2.HOUGH_GRADIENT, dp=1, minDist=50,\n",
    "            param1=50, param2=30, minRadius=10, maxRadius=100\n",
    "        )\n",
    "        \n",
    "        result_image = image.copy()\n",
    "        \n",
    "        if circles is not None:\n",
    "            circles = np.round(circles[0, :]).astype(\"int\")\n",
    "            \n",
    "            for (x, y, r) in circles:\n",
    "                cv2.circle(result_image, (x, y), r, (0, 255, 0), 4)\n",
    "                cv2.circle(result_image, (x, y), 2, (0, 0, 255), 3)\n",
    "        \n",
    "        # Display results\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        axes[0].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        axes[0].set_title('Original Image')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        axes[1].imshow(cv2.cvtColor(result_image, cv2.COLOR_BGR2RGB))\n",
    "        axes[1].set_title(f'Circle Detection ({len(circles) if circles is not None else 0} circles)')\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"üîç Found {len(circles) if circles is not None else 0} circular objects\")\n",
    "    \n",
    "    simple_circle_detection(sample_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18c5e3a",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Section 4: Building Your Own Tools\n",
    "\n",
    "Let's create practical computer vision applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c487fe50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Image Editor\n",
    "class InteractiveImageEditor:\n",
    "    \"\"\"A simple interactive image editor\"\"\"\n",
    "    \n",
    "    def __init__(self, image):\n",
    "        self.original = image.copy()\n",
    "        self.current = image.copy()\n",
    "        self.history = []\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset to original image\"\"\"\n",
    "        self.current = self.original.copy()\n",
    "        self.history = []\n",
    "        print(\"‚úÖ Reset to original image\")\n",
    "    \n",
    "    def apply_blur(self, kernel_size=15):\n",
    "        \"\"\"Apply Gaussian blur\"\"\"\n",
    "        self.current = cv2.GaussianBlur(self.current, (kernel_size, kernel_size), 0)\n",
    "        self.history.append(f\"Blur (kernel={kernel_size})\")\n",
    "        print(f\"‚úÖ Applied blur with kernel size {kernel_size}\")\n",
    "    \n",
    "    def adjust_brightness(self, value=0):\n",
    "        \"\"\"Adjust brightness\"\"\"\n",
    "        self.current = cv2.convertScaleAbs(self.current, alpha=1.0, beta=value)\n",
    "        self.history.append(f\"Brightness {value:+d}\")\n",
    "        print(f\"‚úÖ Adjusted brightness by {value:+d}\")\n",
    "    \n",
    "    def adjust_contrast(self, value=1.0):\n",
    "        \"\"\"Adjust contrast\"\"\"\n",
    "        self.current = cv2.convertScaleAbs(self.current, alpha=value, beta=0)\n",
    "        self.history.append(f\"Contrast x{value}\")\n",
    "        print(f\"‚úÖ Adjusted contrast by factor of {value}\")\n",
    "    \n",
    "    def convert_to_grayscale(self):\n",
    "        \"\"\"Convert to grayscale\"\"\"\n",
    "        gray = cv2.cvtColor(self.current, cv2.COLOR_BGR2GRAY)\n",
    "        self.current = cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)\n",
    "        self.history.append(\"Grayscale conversion\")\n",
    "        print(\"‚úÖ Converted to grayscale\")\n",
    "    \n",
    "    def apply_edge_detection(self):\n",
    "        \"\"\"Apply edge detection\"\"\"\n",
    "        gray = cv2.cvtColor(self.current, cv2.COLOR_BGR2GRAY)\n",
    "        edges = cv2.Canny(gray, 50, 150)\n",
    "        self.current = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)\n",
    "        self.history.append(\"Edge detection\")\n",
    "        print(\"‚úÖ Applied edge detection\")\n",
    "    \n",
    "    def show_current(self):\n",
    "        \"\"\"Display current image\"\"\"\n",
    "        display_image(self.current, \"Current Image\")\n",
    "    \n",
    "    def show_comparison(self):\n",
    "        \"\"\"Show before/after comparison\"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        axes[0].imshow(cv2.cvtColor(self.original, cv2.COLOR_BGR2RGB))\n",
    "        axes[0].set_title('Original')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        axes[1].imshow(cv2.cvtColor(self.current, cv2.COLOR_BGR2RGB))\n",
    "        axes[1].set_title('Edited')\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        if self.history:\n",
    "            print(\"üìù Edit History:\")\n",
    "            for i, edit in enumerate(self.history, 1):\n",
    "                print(f\"   {i}. {edit}\")\n",
    "\n",
    "# Create an image editor instance\n",
    "editor = InteractiveImageEditor(sample_image)\n",
    "\n",
    "print(\"üé® Interactive Image Editor Created!\")\n",
    "print(\"Try these commands:\")\n",
    "print(\"   editor.apply_blur(15)\")\n",
    "print(\"   editor.adjust_brightness(50)\")\n",
    "print(\"   editor.adjust_contrast(1.5)\")\n",
    "print(\"   editor.convert_to_grayscale()\")\n",
    "print(\"   editor.apply_edge_detection()\")\n",
    "print(\"   editor.show_current()\")\n",
    "print(\"   editor.show_comparison()\")\n",
    "print(\"   editor.reset()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499ed5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try out the image editor!\n",
    "# Experiment with different combinations\n",
    "\n",
    "# Example 1: Create a dramatic effect\n",
    "editor.reset()\n",
    "editor.adjust_contrast(1.3)\n",
    "editor.adjust_brightness(20)\n",
    "editor.show_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57abbb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Create an artistic edge effect\n",
    "editor.reset()\n",
    "editor.apply_blur(5)\n",
    "editor.apply_edge_detection()\n",
    "editor.show_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7422fdeb",
   "metadata": {},
   "source": [
    "## üéØ Practice Exercises\n",
    "\n",
    "Now it's your turn! Try these exercises to reinforce your learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a19bd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Color Space Explorer\n",
    "def exercise_color_spaces():\n",
    "    \"\"\"\n",
    "    Exercise: Explore different color spaces\n",
    "    \n",
    "    TODO: Complete this function to:\n",
    "    1. Convert the sample image to HSV color space\n",
    "    2. Split into H, S, V channels\n",
    "    3. Display each channel separately\n",
    "    4. Try to isolate a specific color range\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üé® Exercise 1: Color Space Explorer\")\n",
    "    print(\"Your task: Convert image to HSV and explore channels\")\n",
    "    \n",
    "    # TODO: Your code here\n",
    "    # Hints:\n",
    "    # - Use cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    # - Use cv2.split() to separate channels\n",
    "    # - Use cv2.inRange() to isolate color ranges\n",
    "    \n",
    "    pass\n",
    "\n",
    "# Uncomment to try the exercise\n",
    "# exercise_color_spaces()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8818d6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Build a Motion Detector\n",
    "def exercise_motion_detection():\n",
    "    \"\"\"\n",
    "    Exercise: Create a simple motion detection system\n",
    "    \n",
    "    TODO: Complete this function to:\n",
    "    1. Take two images (or frames)\n",
    "    2. Calculate the difference\n",
    "    3. Threshold the difference to find motion\n",
    "    4. Highlight motion areas\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üèÉ Exercise 2: Motion Detection\")\n",
    "    print(\"Your task: Detect differences between two images\")\n",
    "    \n",
    "    # Create two slightly different images for demo\n",
    "    img1 = sample_image.copy()\n",
    "    img2 = sample_image.copy()\n",
    "    \n",
    "    # Add some \"motion\" to the second image\n",
    "    cv2.circle(img2, (100, 100), 30, (255, 255, 255), -1)\n",
    "    \n",
    "    # TODO: Your code here\n",
    "    # Hints:\n",
    "    # - Convert images to grayscale\n",
    "    # - Use cv2.absdiff() to find differences\n",
    "    # - Use cv2.threshold() to create binary mask\n",
    "    # - Use cv2.findContours() to find motion regions\n",
    "    \n",
    "    pass\n",
    "\n",
    "# Uncomment to try the exercise\n",
    "# exercise_motion_detection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47403d15",
   "metadata": {},
   "source": [
    "## üöÄ Advanced Challenge: Build a Feature Matcher\n",
    "\n",
    "Ready for a challenge? Let's build a system that can match features between two images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbf68bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Challenge: Feature Matching\n",
    "def feature_matching_challenge():\n",
    "    \"\"\"\n",
    "    Advanced Challenge: Build a feature matching system\n",
    "    \n",
    "    This demonstrates how to find corresponding points between two images\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üéØ Advanced Challenge: Feature Matching\")\n",
    "    \n",
    "    # Create two versions of the same image (simulating different viewpoints)\n",
    "    img1 = sample_image.copy()\n",
    "    \n",
    "    # Transform the second image slightly\n",
    "    rows, cols = img1.shape[:2]\n",
    "    M = cv2.getRotationMatrix2D((cols/2, rows/2), 15, 0.9)\n",
    "    img2 = cv2.warpAffine(img1, M, (cols, rows))\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "    gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    try:\n",
    "        # Initialize SIFT detector\n",
    "        sift = cv2.SIFT_create()\n",
    "        \n",
    "        # Find keypoints and descriptors\n",
    "        kp1, des1 = sift.detectAndCompute(gray1, None)\n",
    "        kp2, des2 = sift.detectAndCompute(gray2, None)\n",
    "        \n",
    "        print(f\"üîç Found {len(kp1)} features in image 1\")\n",
    "        print(f\"üîç Found {len(kp2)} features in image 2\")\n",
    "        \n",
    "        # Match features\n",
    "        bf = cv2.BFMatcher()\n",
    "        matches = bf.knnMatch(des1, des2, k=2)\n",
    "        \n",
    "        # Apply ratio test to filter good matches\n",
    "        good_matches = []\n",
    "        for m, n in matches:\n",
    "            if m.distance < 0.75 * n.distance:\n",
    "                good_matches.append([m])\n",
    "        \n",
    "        print(f\"‚úÖ Found {len(good_matches)} good matches\")\n",
    "        \n",
    "        # Draw matches\n",
    "        img_matches = cv2.drawMatchesKnn(\n",
    "            img1, kp1, img2, kp2, good_matches, None,\n",
    "            flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n",
    "        )\n",
    "        \n",
    "        # Display result\n",
    "        plt.figure(figsize=(20, 8))\n",
    "        plt.imshow(cv2.cvtColor(img_matches, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(f'Feature Matching: {len(good_matches)} good matches')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "        # Show individual images with keypoints\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        img1_kp = cv2.drawKeypoints(img1, kp1, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "        img2_kp = cv2.drawKeypoints(img2, kp2, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "        \n",
    "        axes[0].imshow(cv2.cvtColor(img1_kp, cv2.COLOR_BGR2RGB))\n",
    "        axes[0].set_title(f'Image 1: {len(kp1)} keypoints')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        axes[1].imshow(cv2.cvtColor(img2_kp, cv2.COLOR_BGR2RGB))\n",
    "        axes[1].set_title(f'Image 2: {len(kp2)} keypoints')\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error in feature matching: {e}\")\n",
    "        print(\"This might be due to OpenCV version or image characteristics\")\n",
    "\n",
    "# Run the feature matching challenge\n",
    "feature_matching_challenge()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3899e389",
   "metadata": {},
   "source": [
    "## üéì Congratulations!\n",
    "\n",
    "You've completed the Computer Vision Fundamentals notebook! üéâ\n",
    "\n",
    "### What You've Learned:\n",
    "\n",
    "1. **Image Basics**: How computers represent and process images\n",
    "2. **Transformations**: Resizing, rotating, and enhancing images\n",
    "3. **Filtering**: Applying various filters for enhancement and feature extraction\n",
    "4. **Feature Detection**: Finding important patterns and landmarks\n",
    "5. **Object Detection**: Identifying and locating objects in images\n",
    "6. **Practical Applications**: Building real computer vision tools\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Experiment More**: Try the exercises with your own images\n",
    "2. **Build Projects**: Create your own computer vision applications\n",
    "3. **Learn Advanced Topics**: Explore deep learning for computer vision\n",
    "4. **Join Communities**: Connect with other computer vision enthusiasts\n",
    "\n",
    "### Resources for Continued Learning:\n",
    "\n",
    "- **OpenCV Documentation**: https://docs.opencv.org/\n",
    "- **Computer Vision Courses**: Coursera, edX, Udacity\n",
    "- **Research Papers**: arXiv.org computer vision section\n",
    "- **Competitions**: Kaggle computer vision competitions\n",
    "\n",
    "Keep experimenting and building! Computer vision is a rapidly evolving field with endless possibilities. üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c60b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final challenge: Create your own computer vision function!\n",
    "def your_custom_cv_function(image):\n",
    "    \"\"\"\n",
    "    Your turn! Create a custom computer vision function.\n",
    "    \n",
    "    Ideas:\n",
    "    - Image quality scorer\n",
    "    - Artistic filter creator\n",
    "    - Simple object counter\n",
    "    - Color palette extractor\n",
    "    \n",
    "    Be creative!\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Your creative computer vision code here!\n",
    "    \n",
    "    print(\"üé® Your custom computer vision function!\")\n",
    "    print(\"Add your own creative image processing here.\")\n",
    "    \n",
    "    # Example: Simple color palette extractor\n",
    "    # Reshape image to a list of pixels\n",
    "    pixels = image.reshape(-1, 3)\n",
    "    \n",
    "    # Convert to float for better processing\n",
    "    pixels = np.float32(pixels)\n",
    "    \n",
    "    # Use k-means to find dominant colors\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 20, 1.0)\n",
    "    k = 5\n",
    "    _, labels, centers = cv2.kmeans(pixels, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
    "    \n",
    "    # Convert back to uint8\n",
    "    centers = np.uint8(centers)\n",
    "    \n",
    "    print(f\"üé® Dominant colors in the image:\")\n",
    "    for i, color in enumerate(centers):\n",
    "        print(f\"   Color {i+1}: RGB({color[2]}, {color[1]}, {color[0]})\")\n",
    "    \n",
    "    return centers\n",
    "\n",
    "# Try your custom function\n",
    "palette = your_custom_cv_function(sample_image)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
