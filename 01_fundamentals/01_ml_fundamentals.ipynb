{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "478b54b0",
   "metadata": {},
   "source": [
    "# ML/AI Fundamentals - Your First Steps\n",
    "\n",
    "Welcome to your ML journey! This notebook covers essential concepts every ML practitioner needs.\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "- Master linear algebra with NumPy\n",
    "- Understand probability and statistics\n",
    "- Build your first ML model\n",
    "- Visualize data effectively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6de4723",
   "metadata": {},
   "source": [
    "## 1. Linear Algebra Foundations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166e72d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9891ac2c",
   "metadata": {},
   "source": [
    "### Vectors and Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3938331d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectors (1D arrays)\n",
    "vector_a = np.array([1, 2, 3])\n",
    "vector_b = np.array([4, 5, 6])\n",
    "\n",
    "print(\"Vector A:\", vector_a)\n",
    "print(\"Vector B:\", vector_b)\n",
    "\n",
    "# Vector operations\n",
    "print(\"\\nVector Operations:\")\n",
    "print(\"Addition:\", vector_a + vector_b)\n",
    "print(\"Dot product:\", np.dot(vector_a, vector_b))\n",
    "print(\"Magnitude of A:\", np.linalg.norm(vector_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c44db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrices (2D arrays)\n",
    "matrix_A = np.array([[1, 2, 3],\n",
    "                     [4, 5, 6],\n",
    "                     [7, 8, 9]])\n",
    "\n",
    "matrix_B = np.array([[9, 8, 7],\n",
    "                     [6, 5, 4],\n",
    "                     [3, 2, 1]])\n",
    "\n",
    "print(\"Matrix A:\")\n",
    "print(matrix_A)\n",
    "print(\"\\nMatrix B:\")\n",
    "print(matrix_B)\n",
    "\n",
    "# Matrix operations\n",
    "print(\"\\nMatrix multiplication:\")\n",
    "print(np.dot(matrix_A, matrix_B))\n",
    "\n",
    "print(\"\\nTranspose of A:\")\n",
    "print(matrix_A.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3cf990",
   "metadata": {},
   "source": [
    "### ðŸ§  Exercise 1: Vector Operations\n",
    "Create two vectors of your choice and compute:\n",
    "1. Their dot product\n",
    "2. The angle between them (hint: use dot product formula)\n",
    "3. Their cross product (for 3D vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3579a5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Exercise 1 solution space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b38383",
   "metadata": {},
   "source": [
    "## 2. Probability and Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217be9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "data = np.random.normal(100, 15, 1000)  # Mean=100, Std=15, n=1000\n",
    "\n",
    "# Basic statistics\n",
    "print(\"Descriptive Statistics:\")\n",
    "print(f\"Mean: {np.mean(data):.2f}\")\n",
    "print(f\"Median: {np.median(data):.2f}\")\n",
    "print(f\"Standard Deviation: {np.std(data):.2f}\")\n",
    "print(f\"Variance: {np.var(data):.2f}\")\n",
    "\n",
    "# Visualize distribution\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(data, bins=30, density=True, alpha=0.7, color='skyblue')\n",
    "plt.title('Distribution of Data')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(data)\n",
    "plt.title('Box Plot')\n",
    "plt.ylabel('Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed6c9e1",
   "metadata": {},
   "source": [
    "### Probability Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9253852b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Different distributions\n",
    "x = np.linspace(-4, 4, 100)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Normal distribution\n",
    "plt.subplot(1, 3, 1)\n",
    "y_normal = stats.norm.pdf(x, 0, 1)\n",
    "plt.plot(x, y_normal, 'b-', linewidth=2, label='Normal(0,1)')\n",
    "plt.title('Normal Distribution')\n",
    "plt.legend()\n",
    "\n",
    "# Exponential distribution\n",
    "plt.subplot(1, 3, 2)\n",
    "x_exp = np.linspace(0, 5, 100)\n",
    "y_exp = stats.expon.pdf(x_exp, scale=1)\n",
    "plt.plot(x_exp, y_exp, 'r-', linewidth=2, label='Exponential(1)')\n",
    "plt.title('Exponential Distribution')\n",
    "plt.legend()\n",
    "\n",
    "# Uniform distribution\n",
    "plt.subplot(1, 3, 3)\n",
    "x_uniform = np.linspace(-2, 2, 100)\n",
    "y_uniform = stats.uniform.pdf(x_uniform, -1, 2)\n",
    "plt.plot(x_uniform, y_uniform, 'g-', linewidth=2, label='Uniform(-1,1)')\n",
    "plt.title('Uniform Distribution')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af07af7",
   "metadata": {},
   "source": [
    "## 3. Your First Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867cbf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0, \n",
    "                          n_informative=2, n_clusters_per_class=1, random_state=42)\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "df = pd.DataFrame(X, columns=['Feature_1', 'Feature_2'])\n",
    "df['Target'] = y\n",
    "\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nClass distribution:\")\n",
    "print(df['Target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7ccf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Scatter plot colored by class\n",
    "colors = ['red', 'blue']\n",
    "for i, color in enumerate(colors):\n",
    "    mask = df['Target'] == i\n",
    "    plt.scatter(df[mask]['Feature_1'], df[mask]['Feature_2'], \n",
    "               c=color, label=f'Class {i}', alpha=0.6)\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Binary Classification Dataset')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354ec4c1",
   "metadata": {},
   "source": [
    "### Train Your First Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34adff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")\n",
    "\n",
    "# Create and train the model\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nModel Accuracy: {accuracy:.3f}\")\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3129cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the decision boundary\n",
    "def plot_decision_boundary(X, y, model, title):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Create a mesh\n",
    "    h = 0.01\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                        np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Make predictions on the mesh\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot the decision boundary\n",
    "    plt.contourf(xx, yy, Z, alpha=0.4, cmap=plt.cm.RdYlBu)\n",
    "    \n",
    "    # Plot the data points\n",
    "    colors = ['red', 'blue']\n",
    "    for i, color in enumerate(colors):\n",
    "        mask = y == i\n",
    "        plt.scatter(X[mask, 0], X[mask, 1], c=color, label=f'Class {i}', alpha=0.8)\n",
    "    \n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plot decision boundary for test data\n",
    "plot_decision_boundary(X_test, y_test, model, 'Logistic Regression Decision Boundary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27e2466",
   "metadata": {},
   "source": [
    "## 4. Key ML Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94994db2",
   "metadata": {},
   "source": [
    "### Bias-Variance Tradeoff Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36273c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate regression data\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "X_reg = np.random.uniform(-1, 1, n_samples).reshape(-1, 1)\n",
    "y_reg = 1.5 * X_reg.ravel() + 0.5 * X_reg.ravel()**2 + np.random.normal(0, 0.1, n_samples)\n",
    "\n",
    "# Split the data\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.3, random_state=42)\n",
    "\n",
    "# Test different polynomial degrees\n",
    "degrees = [1, 2, 5, 10, 15]\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, degree in enumerate(degrees):\n",
    "    # Create polynomial features\n",
    "    poly_model = Pipeline([\n",
    "        ('poly', PolynomialFeatures(degree=degree)),\n",
    "        ('linear', LinearRegression())\n",
    "    ])\n",
    "    \n",
    "    # Fit the model\n",
    "    poly_model.fit(X_train_reg, y_train_reg)\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred = poly_model.predict(X_train_reg)\n",
    "    y_test_pred = poly_model.predict(X_test_reg)\n",
    "    \n",
    "    # Calculate errors\n",
    "    train_error = mean_squared_error(y_train_reg, y_train_pred)\n",
    "    test_error = mean_squared_error(y_test_reg, y_test_pred)\n",
    "    \n",
    "    train_errors.append(train_error)\n",
    "    test_errors.append(test_error)\n",
    "    \n",
    "    # Plot\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    X_plot = np.linspace(-1, 1, 100).reshape(-1, 1)\n",
    "    y_plot = poly_model.predict(X_plot)\n",
    "    \n",
    "    plt.scatter(X_train_reg, y_train_reg, alpha=0.6, color='blue', label='Train')\n",
    "    plt.scatter(X_test_reg, y_test_reg, alpha=0.6, color='red', label='Test')\n",
    "    plt.plot(X_plot, y_plot, color='green', linewidth=2)\n",
    "    plt.title(f'Degree {degree}\\nTrain MSE: {train_error:.3f}, Test MSE: {test_error:.3f}')\n",
    "    plt.legend()\n",
    "\n",
    "# Plot bias-variance tradeoff\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.plot(degrees, train_errors, 'o-', label='Training Error', color='blue')\n",
    "plt.plot(degrees, test_errors, 'o-', label='Test Error', color='red')\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('Bias-Variance Tradeoff')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe6b240",
   "metadata": {},
   "source": [
    "## ðŸ§  Exercise 2: Model Comparison\n",
    "Try different models on our classification dataset:\n",
    "1. Decision Tree\n",
    "2. Random Forest\n",
    "3. Support Vector Machine\n",
    "\n",
    "Compare their accuracies and visualize their decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36feaca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Your code here\n",
    "# Exercise 2 solution space\n",
    "models = {\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'SVM': SVC(random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    # Fit model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    results[name] = accuracy\n",
    "    \n",
    "    print(f\"{name} Accuracy: {accuracy:.3f}\")\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "models_list = list(results.keys())\n",
    "accuracies = list(results.values())\n",
    "\n",
    "plt.bar(models_list, accuracies, color=['skyblue', 'lightgreen', 'lightcoral'])\n",
    "plt.title('Model Comparison')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "for i, v in enumerate(accuracies):\n",
    "    plt.text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57db11bd",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Key Takeaways\n",
    "\n",
    "Congratulations! You've completed your first ML notebook. Here's what you learned:\n",
    "\n",
    "### âœ… Mathematical Foundations\n",
    "- Vector and matrix operations are fundamental to ML\n",
    "- Understanding probability distributions helps in data analysis\n",
    "- Statistics provide insights into data quality and model performance\n",
    "\n",
    "### âœ… Machine Learning Basics\n",
    "- ML is about finding patterns in data\n",
    "- Train/test split prevents overfitting\n",
    "- Different algorithms have different strengths\n",
    "\n",
    "### âœ… Bias-Variance Tradeoff\n",
    "- Simple models might underfit (high bias)\n",
    "- Complex models might overfit (high variance)\n",
    "- The goal is finding the right balance\n",
    "\n",
    "## ðŸš€ Next Steps\n",
    "\n",
    "1. **Practice**: Try the exercises above\n",
    "2. **Explore**: Modify the code and see what happens\n",
    "3. **Read**: Review the concepts you found challenging\n",
    "4. **Move On**: Ready for Module 02 - Python ML Stack!\n",
    "\n",
    "## ðŸ“š Additional Resources\n",
    "\n",
    "- [Linear Algebra Review](https://www.khanacademy.org/math/linear-algebra)\n",
    "- [Statistics and Probability](https://www.coursera.org/learn/stanford-statistics)\n",
    "- [Scikit-learn Documentation](https://scikit-learn.org/stable/)\n",
    "- [NumPy Tutorial](https://numpy.org/learn/)\n",
    "\n",
    "Great job! You're well on your way to becoming an ML engineer! ðŸŽ‰"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
