{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "271dd1f4",
   "metadata": {},
   "source": [
    "# Calculus & Optimization Fundamentals for Machine Learning\n",
    "\n",
    "This notebook covers the essential calculus and optimization concepts needed for ML/AI:\n",
    "1. **Derivatives**: Partial derivatives, chain rule, gradients\n",
    "2. **Optimization**: Gradient descent, local/global minima\n",
    "3. **Multivariable Calculus**: Gradients, Hessians, Taylor series\n",
    "4. **Constrained Optimization**: Lagrange multipliers\n",
    "\n",
    "## Why Calculus Matters in ML\n",
    "- **Optimization**: Finding best parameters for models\n",
    "- **Backpropagation**: Training neural networks\n",
    "- **Loss minimization**: Core of all learning algorithms\n",
    "- **Convergence analysis**: Understanding when algorithms work\n",
    "- **Feature engineering**: Understanding function behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0c433b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "from scipy.optimize import minimize\n",
    "from scipy import optimize\n",
    "import sympy as sp\n",
    "from sympy import symbols, diff, Matrix, hessian\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üìö Libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"SymPy version: {sp.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778839ff",
   "metadata": {},
   "source": [
    "# 1. Derivatives: The Foundation of Optimization\n",
    "\n",
    "## What is a Derivative?\n",
    "A derivative measures **how a function changes** when its input changes slightly.\n",
    "\n",
    "**Mathematical Definition**:\n",
    "$$f'(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}$$\n",
    "\n",
    "**Geometric Interpretation**: Slope of the tangent line\n",
    "\n",
    "**ML Interpretation**: \n",
    "- **Direction**: Which way to move parameters\n",
    "- **Magnitude**: How much to change parameters\n",
    "- **Optimization**: Follow negative gradient to minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3e9492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing derivatives as slopes\n",
    "def f(x):\n",
    "    \"\"\"Simple quadratic function\"\"\"\n",
    "    return x**2 + 2*x + 1\n",
    "\n",
    "def f_derivative(x):\n",
    "    \"\"\"Derivative of f(x)\"\"\"\n",
    "    return 2*x + 2\n",
    "\n",
    "# Create x values\n",
    "x = np.linspace(-3, 2, 1000)\n",
    "y = f(x)\n",
    "\n",
    "# Points to show tangent lines\n",
    "points = [-2, -1, 0, 1]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot function and tangent lines\n",
    "ax1.plot(x, y, 'b-', linewidth=2, label='f(x) = x¬≤ + 2x + 1')\n",
    "\n",
    "for point in points:\n",
    "    # Function value at point\n",
    "    y_val = f(point)\n",
    "    \n",
    "    # Derivative (slope) at point\n",
    "    slope = f_derivative(point)\n",
    "    \n",
    "    # Tangent line: y - y0 = m(x - x0)\n",
    "    tangent_x = np.linspace(point - 0.5, point + 0.5, 100)\n",
    "    tangent_y = slope * (tangent_x - point) + y_val\n",
    "    \n",
    "    # Plot point and tangent\n",
    "    ax1.plot(point, y_val, 'ro', markersize=8)\n",
    "    ax1.plot(tangent_x, tangent_y, 'r--', alpha=0.7, \n",
    "             label=f'Tangent at x={point}, slope={slope:.1f}')\n",
    "    \n",
    "    # Add slope annotation\n",
    "    ax1.annotate(f'slope = {slope:.1f}', \n",
    "                xy=(point, y_val), xytext=(point + 0.3, y_val + 0.5),\n",
    "                arrowprops=dict(arrowstyle='->', color='red', alpha=0.7),\n",
    "                fontsize=10, ha='center')\n",
    "\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('f(x)')\n",
    "ax1.set_title('Function and Tangent Lines (Derivatives)', fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Plot derivative function\n",
    "ax2.plot(x, f_derivative(x), 'g-', linewidth=2, label=\"f'(x) = 2x + 2\")\n",
    "ax2.axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
    "ax2.axvline(x=-1, color='r', linestyle='--', alpha=0.5, label='Critical point (f\\'(x)=0)')\n",
    "\n",
    "# Mark points where we calculated slopes\n",
    "for point in points:\n",
    "    slope = f_derivative(point)\n",
    "    ax2.plot(point, slope, 'ro', markersize=8)\n",
    "    ax2.annotate(f'({point}, {slope})', \n",
    "                xy=(point, slope), xytext=(point + 0.2, slope + 0.3),\n",
    "                fontsize=10, ha='center')\n",
    "\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel(\"f'(x)\")\n",
    "ax2.set_title('Derivative Function', fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Insights:\")\n",
    "print(\"‚Ä¢ Derivative = 0 at minimum (x = -1)\")\n",
    "print(\"‚Ä¢ Negative derivative ‚Üí function decreasing\")\n",
    "print(\"‚Ä¢ Positive derivative ‚Üí function increasing\")\n",
    "print(\"‚Ä¢ Steeper slope ‚Üí larger derivative magnitude\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985cf567",
   "metadata": {},
   "source": [
    "## Partial Derivatives\n",
    "\n",
    "For functions of **multiple variables** f(x, y), partial derivatives measure how the function changes with respect to **one variable while keeping others constant**.\n",
    "\n",
    "**Notation**:\n",
    "- $\\frac{\\partial f}{\\partial x}$: Partial derivative with respect to x\n",
    "- $\\frac{\\partial f}{\\partial y}$: Partial derivative with respect to y\n",
    "\n",
    "**ML Context**: Most loss functions depend on many parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8a910a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial derivatives example\n",
    "# Define symbolic variables\n",
    "x, y = symbols('x y')\n",
    "\n",
    "# Define a function of two variables\n",
    "f_xy = x**2 + 2*x*y + y**2 + 3*x + 4*y + 5\n",
    "\n",
    "print(\"Function: f(x,y) =\", f_xy)\n",
    "print()\n",
    "\n",
    "# Calculate partial derivatives\n",
    "df_dx = diff(f_xy, x)\n",
    "df_dy = diff(f_xy, y)\n",
    "\n",
    "print(\"Partial Derivatives:\")\n",
    "print(f\"‚àÇf/‚àÇx = {df_dx}\")\n",
    "print(f\"‚àÇf/‚àÇy = {df_dy}\")\n",
    "print()\n",
    "\n",
    "# Find critical points (where both partial derivatives = 0)\n",
    "critical_points = sp.solve([df_dx, df_dy], [x, y])\n",
    "print(f\"Critical point: {critical_points}\")\n",
    "\n",
    "# Convert to numerical functions for plotting\n",
    "f_numeric = sp.lambdify([x, y], f_xy, 'numpy')\n",
    "df_dx_numeric = sp.lambdify([x, y], df_dx, 'numpy')\n",
    "df_dy_numeric = sp.lambdify([x, y], df_dy, 'numpy')\n",
    "\n",
    "# Create mesh for 3D plotting\n",
    "x_vals = np.linspace(-3, 1, 50)\n",
    "y_vals = np.linspace(-3, 1, 50)\n",
    "X, Y = np.meshgrid(x_vals, y_vals)\n",
    "Z = f_numeric(X, Y)\n",
    "\n",
    "# Create subplots\n",
    "fig = plt.figure(figsize=(18, 5))\n",
    "\n",
    "# 3D surface plot\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "surface = ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.7)\n",
    "\n",
    "# Mark critical point\n",
    "cp_x, cp_y = float(critical_points[x]), float(critical_points[y])\n",
    "cp_z = f_numeric(cp_x, cp_y)\n",
    "ax1.scatter([cp_x], [cp_y], [cp_z], color='red', s=100, label='Critical Point')\n",
    "\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "ax1.set_zlabel('f(x,y)')\n",
    "ax1.set_title('Function f(x,y)', fontweight='bold')\n",
    "\n",
    "# Contour plot with gradient field\n",
    "ax2 = fig.add_subplot(132)\n",
    "contour = ax2.contour(X, Y, Z, levels=20)\n",
    "ax2.clabel(contour, inline=True, fontsize=8)\n",
    "\n",
    "# Add gradient vectors (scaled down for visibility)\n",
    "step = 5  # Plot every 5th vector\n",
    "dx = df_dx_numeric(X[::step, ::step], Y[::step, ::step])\n",
    "dy = df_dy_numeric(X[::step, ::step], Y[::step, ::step])\n",
    "\n",
    "ax2.quiver(X[::step, ::step], Y[::step, ::step], -dx, -dy, \n",
    "           alpha=0.6, scale=50, width=0.003, color='red')\n",
    "\n",
    "ax2.plot(cp_x, cp_y, 'ro', markersize=10, label='Critical Point')\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('y')\n",
    "ax2.set_title('Contour Plot + Gradient Field', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Partial derivative cross-sections\n",
    "ax3 = fig.add_subplot(133)\n",
    "\n",
    "# Fix y at critical point and plot f vs x\n",
    "x_line = np.linspace(-3, 1, 100)\n",
    "f_x_fixed_y = f_numeric(x_line, cp_y)\n",
    "ax3.plot(x_line, f_x_fixed_y, 'b-', label=f'f(x, {cp_y:.1f})', linewidth=2)\n",
    "\n",
    "# Fix x at critical point and plot f vs y  \n",
    "y_line = np.linspace(-3, 1, 100)\n",
    "f_y_fixed_x = f_numeric(cp_x, y_line)\n",
    "ax3.plot(y_line, f_y_fixed_x, 'g-', label=f'f({cp_x:.1f}, y)', linewidth=2)\n",
    "\n",
    "ax3.plot(cp_x, cp_z, 'ro', markersize=10, label='Minimum')\n",
    "ax3.set_xlabel('x or y')\n",
    "ax3.set_ylabel('f')\n",
    "ax3.set_title('Cross-sections through Critical Point', fontweight='bold')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAt critical point ({cp_x:.1f}, {cp_y:.1f}):\")\n",
    "print(f\"‚àÇf/‚àÇx = {df_dx_numeric(cp_x, cp_y):.10f}\")\n",
    "print(f\"‚àÇf/‚àÇy = {df_dy_numeric(cp_x, cp_y):.10f}\")\n",
    "print(f\"Function value = {cp_z:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65d22a3",
   "metadata": {},
   "source": [
    "## Chain Rule: The Backbone of Backpropagation\n",
    "\n",
    "The chain rule tells us how to find derivatives of **composite functions**.\n",
    "\n",
    "**Single Variable**:\n",
    "If $y = f(g(x))$, then $\\frac{dy}{dx} = f'(g(x)) \\cdot g'(x)$\n",
    "\n",
    "**Multivariable**:\n",
    "If $z = f(x, y)$ where $x = x(t)$ and $y = y(t)$, then:\n",
    "$$\\frac{dz}{dt} = \\frac{\\partial z}{\\partial x}\\frac{dx}{dt} + \\frac{\\partial z}{\\partial y}\\frac{dy}{dt}$$\n",
    "\n",
    "**ML Context**: This is exactly how backpropagation works in neural networks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352a0a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain rule demonstration\n",
    "print(\"üîó CHAIN RULE EXAMPLES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Example 1: Simple composition\n",
    "print(\"\\n1. Single Variable Chain Rule\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Define symbolic functions\n",
    "t = symbols('t')\n",
    "\n",
    "# Let u = t^2 + 1, and y = sin(u)\n",
    "u = t**2 + 1\n",
    "y = sp.sin(u)\n",
    "\n",
    "print(f\"u(t) = {u}\")\n",
    "print(f\"y(u) = sin(u)\")\n",
    "print(f\"Composite: y(t) = {y}\")\n",
    "print()\n",
    "\n",
    "# Calculate derivatives\n",
    "du_dt = diff(u, t)\n",
    "dy_du = sp.cos(u)  # derivative of sin(u)\n",
    "dy_dt_chain = dy_du * du_dt\n",
    "dy_dt_direct = diff(y, t)\n",
    "\n",
    "print(f\"du/dt = {du_dt}\")\n",
    "print(f\"dy/du = {dy_du}\")\n",
    "print(f\"dy/dt (chain rule) = (dy/du)(du/dt) = {dy_dt_chain}\")\n",
    "print(f\"dy/dt (direct) = {dy_dt_direct}\")\n",
    "print(f\"Same result? {sp.simplify(dy_dt_chain - dy_dt_direct) == 0}\")\n",
    "\n",
    "# Example 2: Neural Network Layer\n",
    "print(\"\\n\\n2. Neural Network Example\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Simple neural network: z = œÉ(wx + b) where œÉ(x) = 1/(1 + e^(-x))\n",
    "w, x_var, b = symbols('w x b')\n",
    "\n",
    "# Linear combination\n",
    "linear = w * x_var + b\n",
    "\n",
    "# Sigmoid activation\n",
    "sigmoid = 1 / (1 + sp.exp(-linear))\n",
    "\n",
    "print(f\"Linear: z = {linear}\")\n",
    "print(f\"Output: a = œÉ(z) = {sigmoid}\")\n",
    "print()\n",
    "\n",
    "# Calculate gradients (what backprop computes!)\n",
    "da_dw = diff(sigmoid, w)\n",
    "da_db = diff(sigmoid, b)\n",
    "da_dx = diff(sigmoid, x_var)\n",
    "\n",
    "print(\"Gradients (for backpropagation):\")\n",
    "print(f\"‚àÇa/‚àÇw = {da_dw}\")\n",
    "print(f\"‚àÇa/‚àÇb = {da_db}\")\n",
    "print(f\"‚àÇa/‚àÇx = {da_dx}\")\n",
    "\n",
    "# Show how chain rule breaks down the computation\n",
    "da_dz = diff(sigmoid, linear)  # derivative of sigmoid\n",
    "dz_dw = diff(linear, w)        # derivative of linear w.r.t. w\n",
    "\n",
    "print(f\"\\nChain rule breakdown:\")\n",
    "print(f\"‚àÇa/‚àÇz = {da_dz}\")\n",
    "print(f\"‚àÇz/‚àÇw = {dz_dw}\")\n",
    "print(f\"‚àÇa/‚àÇw = (‚àÇa/‚àÇz)(‚àÇz/‚àÇw) = {sp.simplify(da_dz * dz_dw)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6712a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing chain rule in action\n",
    "def sigmoid_func(x):\n",
    "    \"\"\"Sigmoid activation function\"\"\"\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))  # Clip to prevent overflow\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"Derivative of sigmoid\"\"\"\n",
    "    s = sigmoid_func(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Parameters for our neural network layer\n",
    "w_val = 2.0\n",
    "b_val = -1.0\n",
    "x_range = np.linspace(-2, 3, 1000)\n",
    "\n",
    "# Forward pass\n",
    "z_vals = w_val * x_range + b_val  # Linear combination\n",
    "a_vals = sigmoid_func(z_vals)     # Activation\n",
    "\n",
    "# Plot 1: Input to linear output\n",
    "axes[0,0].plot(x_range, z_vals, 'b-', linewidth=2, label=f'z = {w_val}x + {b_val}')\n",
    "axes[0,0].set_xlabel('Input x')\n",
    "axes[0,0].set_ylabel('Linear output z')\n",
    "axes[0,0].set_title('Step 1: Linear Transformation', fontweight='bold')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "axes[0,0].legend()\n",
    "\n",
    "# Plot 2: Linear output to final activation\n",
    "z_plot_range = np.linspace(-6, 6, 1000)\n",
    "a_plot_vals = sigmoid_func(z_plot_range)\n",
    "axes[0,1].plot(z_plot_range, a_plot_vals, 'g-', linewidth=2, label='a = œÉ(z)')\n",
    "axes[0,1].set_xlabel('Linear output z')\n",
    "axes[0,1].set_ylabel('Activation a')\n",
    "axes[0,1].set_title('Step 2: Sigmoid Activation', fontweight='bold')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "axes[0,1].legend()\n",
    "\n",
    "# Plot 3: Complete input to output transformation\n",
    "axes[1,0].plot(x_range, a_vals, 'r-', linewidth=2, label='a = œÉ(wx + b)')\n",
    "axes[1,0].set_xlabel('Input x')\n",
    "axes[1,0].set_ylabel('Final output a')\n",
    "axes[1,0].set_title('Complete Transformation', fontweight='bold')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "axes[1,0].legend()\n",
    "\n",
    "# Plot 4: Gradients (chain rule in action)\n",
    "da_dz_vals = sigmoid_derivative(z_vals)  # ‚àÇa/‚àÇz\n",
    "dz_dx_vals = np.full_like(x_range, w_val)  # ‚àÇz/‚àÇx = w\n",
    "da_dx_vals = da_dz_vals * dz_dx_vals  # Chain rule: ‚àÇa/‚àÇx = (‚àÇa/‚àÇz)(‚àÇz/‚àÇx)\n",
    "\n",
    "axes[1,1].plot(x_range, da_dz_vals, 'orange', linewidth=2, label='‚àÇa/‚àÇz (sigmoid derivative)')\n",
    "axes[1,1].axhline(y=w_val, color='blue', linestyle='--', linewidth=2, label=f'‚àÇz/‚àÇx = {w_val}')\n",
    "axes[1,1].plot(x_range, da_dx_vals, 'red', linewidth=2, label='‚àÇa/‚àÇx (chain rule)')\n",
    "axes[1,1].set_xlabel('Input x')\n",
    "axes[1,1].set_ylabel('Gradient values')\n",
    "axes[1,1].set_title('Gradients via Chain Rule', fontweight='bold')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "axes[1,1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üß† Neural Network Insights:\")\n",
    "print(\"‚Ä¢ Forward pass: x ‚Üí z = wx + b ‚Üí a = œÉ(z)\")\n",
    "print(\"‚Ä¢ Backward pass: ‚àÇa/‚àÇx = (‚àÇa/‚àÇz)(‚àÇz/‚àÇx) = œÉ'(z) √ó w\")\n",
    "print(\"‚Ä¢ Chain rule enables efficient gradient computation\")\n",
    "print(\"‚Ä¢ This is exactly how backpropagation works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794f1aa3",
   "metadata": {},
   "source": [
    "## Gradients: The Direction of Steepest Ascent\n",
    "\n",
    "The **gradient** is a vector containing all partial derivatives of a function.\n",
    "\n",
    "**Definition**:\n",
    "$$\\nabla f(x, y) = \\begin{bmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{bmatrix}$$\n",
    "\n",
    "**Key Properties**:\n",
    "- Points in direction of **steepest increase**\n",
    "- **Magnitude** indicates how steep the increase is\n",
    "- **Perpendicular** to level curves/contours\n",
    "- **Zero gradient** = critical point (local min/max/saddle)\n",
    "\n",
    "**ML Applications**:\n",
    "- **Gradient descent**: Move opposite to gradient\n",
    "- **Feature importance**: Large gradients = sensitive parameters\n",
    "- **Optimization**: Follow gradients to extrema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eca4eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive gradient visualization\n",
    "def create_test_function(func_type='bowl'):\n",
    "    \"\"\"Create different types of functions for gradient analysis\"\"\"\n",
    "    if func_type == 'bowl':\n",
    "        return lambda x, y: x**2 + y**2 + 0.5*x*y\n",
    "    elif func_type == 'saddle':\n",
    "        return lambda x, y: x**2 - y**2\n",
    "    elif func_type == 'rosenbrock':\n",
    "        return lambda x, y: (1 - x)**2 + 100*(y - x**2)**2\n",
    "    elif func_type == 'peaks':\n",
    "        return lambda x, y: 3*(1-x)**2 * np.exp(-(x**2) - (y+1)**2) - 10*(x/5 - x**3 - y**5) * np.exp(-x**2-y**2) - (1/3)*np.exp(-(x+1)**2 - y**2)\n",
    "\n",
    "def numerical_gradient(f, x, y, h=1e-5):\n",
    "    \"\"\"Compute gradient numerically using finite differences\"\"\"\n",
    "    grad_x = (f(x + h, y) - f(x - h, y)) / (2 * h)\n",
    "    grad_y = (f(x, y + h) - f(x, y - h)) / (2 * h)\n",
    "    return grad_x, grad_y\n",
    "\n",
    "# Test different function types\n",
    "functions = {\n",
    "    'Convex Bowl': 'bowl',\n",
    "    'Saddle Point': 'saddle', \n",
    "    'Rosenbrock': 'rosenbrock'\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "for i, (name, func_type) in enumerate(functions.items()):\n",
    "    f = create_test_function(func_type)\n",
    "    \n",
    "    # Create coordinate grids\n",
    "    if func_type == 'rosenbrock':\n",
    "        x_range = np.linspace(-2, 2, 50)\n",
    "        y_range = np.linspace(-1, 3, 50)\n",
    "    else:\n",
    "        x_range = np.linspace(-3, 3, 50)\n",
    "        y_range = np.linspace(-3, 3, 50)\n",
    "    \n",
    "    X, Y = np.meshgrid(x_range, y_range)\n",
    "    Z = f(X, Y)\n",
    "    \n",
    "    # Create contour plot\n",
    "    contour = axes[i].contour(X, Y, Z, levels=15, alpha=0.6)\n",
    "    axes[i].contourf(X, Y, Z, levels=15, alpha=0.3, cmap='viridis')\n",
    "    \n",
    "    # Add gradient field\n",
    "    step = 5  # Sample every 5th point\n",
    "    X_sample = X[::step, ::step]\n",
    "    Y_sample = Y[::step, ::step]\n",
    "    \n",
    "    # Compute gradients\n",
    "    grad_x = np.zeros_like(X_sample)\n",
    "    grad_y = np.zeros_like(Y_sample)\n",
    "    \n",
    "    for row in range(X_sample.shape[0]):\n",
    "        for col in range(X_sample.shape[1]):\n",
    "            gx, gy = numerical_gradient(f, X_sample[row, col], Y_sample[row, col])\n",
    "            grad_x[row, col] = gx\n",
    "            grad_y[row, col] = gy\n",
    "    \n",
    "    # Plot gradient vectors\n",
    "    axes[i].quiver(X_sample, Y_sample, grad_x, grad_y, \n",
    "                   angles='xy', scale_units='xy', scale=None,\n",
    "                   color='red', alpha=0.7, width=0.003)\n",
    "    \n",
    "    # Find and mark critical points (where gradient ‚âà 0)\n",
    "    if func_type == 'bowl':\n",
    "        # For x¬≤ + y¬≤ + 0.5xy, critical point is at (0, 0)\n",
    "        axes[i].plot(0, 0, 'ro', markersize=10, label='Minimum')\n",
    "    elif func_type == 'saddle':\n",
    "        # For x¬≤ - y¬≤, critical point is at (0, 0)\n",
    "        axes[i].plot(0, 0, 'bo', markersize=10, label='Saddle Point')\n",
    "    elif func_type == 'rosenbrock':\n",
    "        # Rosenbrock minimum is at (1, 1)\n",
    "        axes[i].plot(1, 1, 'go', markersize=10, label='Global Minimum')\n",
    "    \n",
    "    axes[i].set_xlabel('x')\n",
    "    axes[i].set_ylabel('y')\n",
    "    axes[i].set_title(f'{name}\\nGradient Field', fontweight='bold')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    axes[i].set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üéØ Gradient Insights:\")\n",
    "print(\"‚Ä¢ Red arrows show gradient direction (steepest ascent)\")\n",
    "print(\"‚Ä¢ To minimize: move OPPOSITE to gradient (gradient descent)\")\n",
    "print(\"‚Ä¢ Arrow length ‚àù gradient magnitude (steepness)\")\n",
    "print(\"‚Ä¢ Zero gradient = critical point (min/max/saddle)\")\n",
    "print(\"‚Ä¢ Gradients perpendicular to contour lines\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
