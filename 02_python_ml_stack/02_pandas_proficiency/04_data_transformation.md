# Data Transformation: Reshaping Your Data for Insights

## ðŸ¤” What is Data Transformation?

Imagine you're a chef with ingredients scattered everywhere - some in the fridge, some in the pantry, some chopped, some whole. **Data transformation is like organizing and preparing your ingredients** so you can cook an amazing meal (analysis)!

Raw data rarely comes in the exact format you need for analysis. Data transformation is the art of reshaping, combining, and restructuring data to reveal insights.

## ðŸŽ¯ Why Data Transformation Matters

**Real-world scenarios where transformation is crucial:**

- **Merging customer data** from website, mobile app, and store purchases
- **Pivoting sales data** from long format to wide format for analysis
- **Grouping transactions** by customer to calculate lifetime value
- **Reshaping survey responses** for statistical analysis
- **Combining time series** data from different sources

## ðŸ§  Core Transformation Concepts

### 1. **Groupby Operations: The Data Analyst's Swiss Army Knife**

Think of groupby like **organizing your music library**:
- Group songs by artist
- Calculate total playtime per artist
- Find the most popular genre

```python
import pandas as pd
import numpy as np

# Create sample e-commerce data
np.random.seed(42)
orders = pd.DataFrame({
    'order_id': range(1, 1001),
    'customer_id': np.random.randint(1, 201, 1000),
    'product_category': np.random.choice(['Electronics', 'Clothing', 'Books', 'Home'], 1000),
    'order_amount': np.random.exponential(50, 1000),
    'order_date': pd.date_range('2023-01-01', periods=1000, freq='H'),
    'region': np.random.choice(['North', 'South', 'East', 'West'], 1000)
})

print("ðŸ“Š GROUPBY OPERATIONS MASTERCLASS")
print("=" * 40)

# Basic groupby: Average order value by category
print("1. Average Order Value by Category:")
avg_order_by_category = orders.groupby('product_category')['order_amount'].mean()
print(avg_order_by_category.round(2))

print("\n2. Multiple Statistics at Once:")
category_stats = orders.groupby('product_category').agg({
    'order_amount': ['count', 'mean', 'sum', 'std'],
    'customer_id': 'nunique'  # Number of unique customers
})

# Flatten column names for readability
category_stats.columns = ['order_count', 'avg_amount', 'total_revenue', 'amount_std', 'unique_customers']
print(category_stats.round(2))

print("\n3. Business Intelligence: Customer Segmentation")
# Group by customer to calculate customer metrics
customer_metrics = orders.groupby('customer_id').agg({
    'order_amount': ['sum', 'mean', 'count'],
    'order_date': ['min', 'max']
}).round(2)

# Flatten columns
customer_metrics.columns = ['total_spent', 'avg_order', 'order_count', 'first_order', 'last_order']

# Calculate customer lifetime (days between first and last order)
customer_metrics['customer_lifetime_days'] = (
    customer_metrics['last_order'] - customer_metrics['first_order']
).dt.days

# Create customer value segments
customer_metrics['value_segment'] = pd.cut(
    customer_metrics['total_spent'], 
    bins=4, 
    labels=['Low', 'Medium', 'High', 'Premium']
)

print("Top 10 Customers by Total Spent:")
print(customer_metrics.nlargest(10, 'total_spent')[['total_spent', 'order_count', 'value_segment']])
```

### 2. **Pivot Tables: Spreadsheet Superpowers in Python**

Pivot tables are like **reorganizing your closet by season AND color** - you can see patterns from multiple perspectives.

```python
print("\nðŸ“Š PIVOT TABLE MASTERY")
print("=" * 30)

# Create a more detailed dataset for pivoting
detailed_orders = orders.copy()
detailed_orders['month'] = detailed_orders['order_date'].dt.month
detailed_orders['quarter'] = detailed_orders['order_date'].dt.quarter

# Basic pivot: Revenue by category and region
revenue_pivot = detailed_orders.pivot_table(
    values='order_amount',
    index='product_category',
    columns='region',
    aggfunc='sum',
    fill_value=0
).round(2)

print("Revenue by Category and Region:")
print(revenue_pivot)

# Advanced pivot: Multiple metrics
advanced_pivot = detailed_orders.pivot_table(
    values='order_amount',
    index='product_category',
    columns='quarter',
    aggfunc=['sum', 'count', 'mean'],
    fill_value=0
).round(2)

print("\nAdvanced Pivot: Sum, Count, and Mean by Quarter:")
print(advanced_pivot)

# Business insight: Which category-region combination is most profitable?
revenue_pivot_melted = revenue_pivot.reset_index().melt(
    id_vars='product_category',
    var_name='region',
    value_name='revenue'
)

best_combinations = revenue_pivot_melted.nlargest(5, 'revenue')
print("\nTop 5 Category-Region Combinations by Revenue:")
print(best_combinations)
```

### 3. **Merging and Joining: Combining Data Sources**

Think of merging like **introducing people at a party** - you want to connect the right people based on what they have in common.

```python
print("\nðŸ”— MERGING DATA SOURCES")
print("=" * 28)

# Create customer demographic data (from CRM system)
customers = pd.DataFrame({
    'customer_id': range(1, 101),
    'name': [f'Customer_{i}' for i in range(1, 101)],
    'age': np.random.randint(18, 70, 100),
    'city': np.random.choice(['NYC', 'LA', 'Chicago', 'Houston'], 100),
    'signup_source': np.random.choice(['Website', 'Mobile', 'Store', 'Referral'], 100)
})

# Create product information (from inventory system)
products = pd.DataFrame({
    'product_category': ['Electronics', 'Clothing', 'Books', 'Home'],
    'profit_margin': [0.15, 0.45, 0.25, 0.35],
    'return_rate': [0.08, 0.12, 0.03, 0.06]
})

print("1. Inner Join: Orders with Customer Demographics")
# Only customers who have placed orders
orders_with_customers = orders.merge(
    customers, 
    on='customer_id', 
    how='inner'
)

print(f"Original orders: {len(orders)}")
print(f"Orders with customer data: {len(orders_with_customers)}")
print("\nSample merged data:")
print(orders_with_customers[['customer_id', 'name', 'age', 'city', 'order_amount']].head())

print("\n2. Left Join: All Orders with Product Information")
# Keep all orders, add product info where available
orders_with_products = orders.merge(
    products,
    on='product_category',
    how='left'
)

# Calculate profitability
orders_with_products['profit'] = orders_with_products['order_amount'] * orders_with_products['profit_margin']

print("Profitability by Category:")
profit_by_category = orders_with_products.groupby('product_category')['profit'].sum().round(2)
print(profit_by_category)

print("\n3. Complex Join: Complete Business View")
# Combine everything for complete business intelligence
complete_data = (orders
                .merge(customers, on='customer_id', how='left')
                .merge(products, on='product_category', how='left'))

# Calculate customer lifetime value
clv_analysis = complete_data.groupby(['customer_id', 'name', 'city']).agg({
    'order_amount': 'sum',
    'profit': 'sum',
    'order_id': 'count'
}).round(2)

clv_analysis.columns = ['total_revenue', 'total_profit', 'order_count']
clv_analysis['avg_order_value'] = clv_analysis['total_revenue'] / clv_analysis['order_count']

print("Top 10 Customers by Profit:")
print(clv_analysis.nlargest(10, 'total_profit'))
```

### 4. **Reshaping Data: From Wide to Long and Back**

Data reshaping is like **rearranging furniture** - same pieces, different layout for different purposes.

```python
print("\nðŸ”„ DATA RESHAPING MASTERY")
print("=" * 32)

# Create quarterly sales data (wide format)
quarterly_sales = pd.DataFrame({
    'product': ['Laptop', 'Phone', 'Tablet', 'Headphones'],
    'Q1_2023': [100000, 150000, 80000, 30000],
    'Q2_2023': [120000, 160000, 85000, 35000],
    'Q3_2023': [110000, 170000, 90000, 40000],
    'Q4_2023': [130000, 180000, 95000, 45000]
})

print("Original Wide Format (good for reading):")
print(quarterly_sales)

# Melt to long format (good for analysis)
sales_long = quarterly_sales.melt(
    id_vars='product',
    var_name='quarter',
    value_name='sales'
)

print("\nLong Format (good for analysis and plotting):")
print(sales_long.head(8))

# Analysis is easier in long format
print("\nAnalysis: Average sales by quarter across all products:")
quarterly_avg = sales_long.groupby('quarter')['sales'].mean().round(2)
print(quarterly_avg)

print("\nAnalysis: Which product has most consistent sales?")
sales_consistency = sales_long.groupby('product')['sales'].std().round(2)
print(sales_consistency.sort_values())

# Pivot back to wide format (for reporting)
sales_pivot = sales_long.pivot(
    index='product',
    columns='quarter',
    values='sales'
)

print("\nPivoted Back to Wide Format:")
print(sales_pivot)
```

### 5. **Advanced Transformations: Custom Business Logic**

```python
print("\nâš™ï¸ ADVANCED TRANSFORMATIONS")
print("=" * 33)

# Apply custom business logic with transform and apply
def calculate_customer_scores(group):
    """Calculate custom customer scoring based on multiple factors"""
    
    # Recency: How recently did they purchase?
    days_since_last_order = (pd.Timestamp.now() - group['order_date'].max()).days
    recency_score = max(0, 100 - days_since_last_order)
    
    # Frequency: How often do they purchase?
    frequency_score = min(100, group['order_id'].count() * 10)
    
    # Monetary: How much do they spend?
    monetary_score = min(100, group['order_amount'].sum() / 1000)
    
    # Combine scores
    total_score = (recency_score * 0.3 + frequency_score * 0.4 + monetary_score * 0.3)
    
    return pd.Series({
        'recency_score': recency_score,
        'frequency_score': frequency_score,
        'monetary_score': monetary_score,
        'total_customer_score': total_score,
        'customer_tier': 'Gold' if total_score > 80 else 'Silver' if total_score > 60 else 'Bronze'
    })

# Apply custom scoring to each customer
customer_scores = complete_data.groupby('customer_id').apply(calculate_customer_scores)
print("Customer Scoring Results (Top 10):")
print(customer_scores.nlargest(10, 'total_customer_score'))

# Window functions: Rolling calculations
print("\nðŸ“ˆ ROLLING CALCULATIONS")
print("=" * 25)

# Calculate rolling 7-day average for daily sales
daily_sales = complete_data.groupby(complete_data['order_date'].dt.date)['order_amount'].sum().reset_index()
daily_sales.columns = ['date', 'daily_revenue']

# Add rolling statistics
daily_sales['7_day_avg'] = daily_sales['daily_revenue'].rolling(window=7).mean()
daily_sales['7_day_max'] = daily_sales['daily_revenue'].rolling(window=7).max()
daily_sales['revenue_trend'] = daily_sales['daily_revenue'] / daily_sales['7_day_avg']

print("Daily Sales with Rolling Metrics (Last 10 days):")
print(daily_sales.tail(10).round(2))

# Identify trending days
trending_up = daily_sales[daily_sales['revenue_trend'] > 1.2]
print(f"\nDays with >20% above 7-day average: {len(trending_up)}")
```

## ðŸŽ­ Transformation Patterns for Business Analytics

### Pattern 1: Customer Analytics Pipeline

```python
def customer_analytics_pipeline(orders_df, customers_df):
    """Complete customer analytics transformation pipeline"""
    
    # Step 1: Enrich orders with customer data
    enriched_orders = orders_df.merge(customers_df, on='customer_id', how='left')
    
    # Step 2: Calculate customer metrics
    customer_summary = enriched_orders.groupby(['customer_id', 'name', 'city']).agg({
        'order_amount': ['sum', 'mean', 'count'],
        'order_date': ['min', 'max']
    })
    
    # Flatten column names
    customer_summary.columns = ['total_spent', 'avg_order', 'order_count', 'first_order', 'last_order']
    
    # Step 3: Calculate business metrics
    customer_summary['customer_lifetime_days'] = (
        customer_summary['last_order'] - customer_summary['first_order']
    ).dt.days + 1
    
    customer_summary['orders_per_month'] = (
        customer_summary['order_count'] / 
        (customer_summary['customer_lifetime_days'] / 30.44)  # Average days per month
    ).fillna(customer_summary['order_count'])  # Handle single-order customers
    
    # Step 4: Create customer segments
    customer_summary['value_tier'] = pd.qcut(
        customer_summary['total_spent'], 
        q=4, 
        labels=['Bronze', 'Silver', 'Gold', 'Platinum']
    )
    
    customer_summary['frequency_tier'] = pd.qcut(
        customer_summary['orders_per_month'], 
        q=3, 
        labels=['Low', 'Medium', 'High']
    )
    
    return customer_summary

# Apply the pipeline
customer_analytics = customer_analytics_pipeline(complete_data, customers)
print("\nðŸŽ¯ CUSTOMER ANALYTICS PIPELINE RESULTS")
print("=" * 45)
print("Customer Distribution by Value Tier:")
print(customer_analytics['value_tier'].value_counts())

print("\nTop Customers by Each Metric:")
print("Highest Spenders:")
print(customer_analytics.nlargest(5, 'total_spent')[['total_spent', 'order_count', 'value_tier']])

print("\nMost Frequent Buyers:")
print(customer_analytics.nlargest(5, 'orders_per_month')[['orders_per_month', 'total_spent', 'frequency_tier']])
```

### Pattern 2: Time Series Aggregation

```python
print("\nðŸ“… TIME SERIES TRANSFORMATION")
print("=" * 35)

# Transform transaction data for time series analysis
def time_series_transformation(df):
    """Transform transactional data for time series analysis"""
    
    # Create date-based features
    df = df.copy()
    df['date'] = df['order_date'].dt.date
    df['month'] = df['order_date'].dt.to_period('M')
    df['week'] = df['order_date'].dt.to_period('W')
    df['day_of_week'] = df['order_date'].dt.day_name()
    df['hour'] = df['order_date'].dt.hour
    
    # Daily aggregations
    daily_metrics = df.groupby('date').agg({
        'order_amount': ['sum', 'mean', 'count'],
        'customer_id': 'nunique'
    })
    
    daily_metrics.columns = ['daily_revenue', 'avg_order_value', 'order_count', 'unique_customers']
    
    # Weekly aggregations
    weekly_metrics = df.groupby('week').agg({
        'order_amount': 'sum',
        'customer_id': 'nunique'
    })
    
    weekly_metrics.columns = ['weekly_revenue', 'weekly_customers']
    
    # Monthly aggregations
    monthly_metrics = df.groupby('month').agg({
        'order_amount': 'sum',
        'customer_id': 'nunique'
    })
    
    monthly_metrics.columns = ['monthly_revenue', 'monthly_customers']
    
    return daily_metrics, weekly_metrics, monthly_metrics

daily, weekly, monthly = time_series_transformation(complete_data)

print("Daily Revenue Trends (Last 10 days):")
print(daily.tail(10))

print("\nBusiness Insights from Time Series:")
print(f"Average daily revenue: ${daily['daily_revenue'].mean():,.2f}")
print(f"Highest single day: ${daily['daily_revenue'].max():,.2f}")
print(f"Most active day customers: {daily['unique_customers'].max()}")

# Seasonal analysis
seasonal_analysis = complete_data.copy()
seasonal_analysis['day_of_week'] = seasonal_analysis['order_date'].dt.day_name()
seasonal_analysis['hour'] = seasonal_analysis['order_date'].dt.hour

print("\nSeasonal Patterns:")
print("Revenue by Day of Week:")
daily_pattern = seasonal_analysis.groupby('day_of_week')['order_amount'].sum().round(2)
print(daily_pattern)

print("\nRevenue by Hour of Day:")
hourly_pattern = seasonal_analysis.groupby('hour')['order_amount'].sum().round(2)
peak_hour = hourly_pattern.idxmax()
print(f"Peak sales hour: {peak_hour}:00 (${hourly_pattern[peak_hour]:,.2f})")
```

### Pattern 3: Complex Multi-Source Integration

```python
print("\nðŸ”— MULTI-SOURCE DATA INTEGRATION")
print("=" * 40)

# Simulate multiple business data sources
# Source 1: Customer demographics (CRM)
crm_data = pd.DataFrame({
    'customer_id': range(1, 201),
    'age': np.random.randint(18, 70, 200),
    'income': np.random.normal(55000, 20000, 200),
    'acquisition_channel': np.random.choice(['Organic', 'Paid', 'Referral', 'Social'], 200)
})

# Source 2: Website behavior (Analytics)
web_behavior = pd.DataFrame({
    'customer_id': np.random.choice(range(1, 201), 500),
    'page_views': np.random.poisson(10, 500),
    'session_duration': np.random.exponential(300, 500),  # seconds
    'session_date': pd.date_range('2023-01-01', periods=500, freq='H')
})

# Source 3: Support tickets (Customer service)
support_tickets = pd.DataFrame({
    'customer_id': np.random.choice(range(1, 201), 150),
    'ticket_type': np.random.choice(['Technical', 'Billing', 'General'], 150),
    'resolution_time': np.random.exponential(24, 150),  # hours
    'satisfaction_rating': np.random.randint(1, 6, 150)
})

def integrate_all_sources(orders, crm, web, support):
    """Integrate data from multiple business systems"""
    
    print("Integration Step 1: Customer Order Summary")
    # Start with customer order summary
    customer_base = orders.groupby('customer_id').agg({
        'order_amount': ['sum', 'count', 'mean'],
        'order_date': ['min', 'max']
    })
    
    customer_base.columns = ['total_spent', 'order_count', 'avg_order', 'first_order', 'last_order']
    customer_base = customer_base.reset_index()
    
    print("Integration Step 2: Add Demographics")
    # Add customer demographics
    integrated = customer_base.merge(crm, on='customer_id', how='left')
    
    print("Integration Step 3: Add Web Behavior")
    # Aggregate web behavior per customer
    web_summary = web.groupby('customer_id').agg({
        'page_views': 'mean',
        'session_duration': 'mean'
    }).round(2)
    
    web_summary.columns = ['avg_page_views', 'avg_session_duration']
    integrated = integrated.merge(web_summary, on='customer_id', how='left')
    
    print("Integration Step 4: Add Support History")
    # Aggregate support tickets per customer
    support_summary = support.groupby('customer_id').agg({
        'resolution_time': 'mean',
        'satisfaction_rating': 'mean',
        'ticket_type': 'count'
    }).round(2)
    
    support_summary.columns = ['avg_resolution_time', 'avg_satisfaction', 'support_tickets_count']
    integrated = integrated.merge(support_summary, on='customer_id', how='left')
    
    # Fill missing values (customers who haven't contacted support)
    integrated['support_tickets_count'] = integrated['support_tickets_count'].fillna(0)
    integrated['avg_satisfaction'] = integrated['avg_satisfaction'].fillna(5)  # Assume satisfied if no tickets
    
    return integrated

# Perform integration
complete_customer_view = integrate_all_sources(orders, crm_data, web_behavior, support_tickets)

print("\nComplete Customer 360 View (Sample):")
print(complete_customer_view.head())

print(f"\nIntegration Results:")
print(f"Total customers: {len(complete_customer_view)}")
print(f"Data sources combined: 4 (Orders, CRM, Web, Support)")
print(f"Features created: {len(complete_customer_view.columns)}")
```

## ðŸ§ª Advanced Transformation Techniques

### Custom Aggregation Functions

```python
def advanced_aggregations():
    """Demonstrate custom aggregation functions"""
    
    print("\nðŸŽ¯ CUSTOM AGGREGATION FUNCTIONS")
    print("=" * 38)
    
    # Create function to calculate business metrics
    def customer_health_score(group):
        """Calculate a custom customer health score"""
        
        recent_orders = group[group['order_date'] > group['order_date'].max() - pd.Timedelta(days=90)]
        
        metrics = {
            'total_orders': len(group),
            'recent_orders': len(recent_orders),
            'total_revenue': group['order_amount'].sum(),
            'avg_order_value': group['order_amount'].mean(),
            'days_since_last_order': (pd.Timestamp.now() - group['order_date'].max()).days,
            'order_frequency': len(group) / max(1, (group['order_date'].max() - group['order_date'].min()).days / 30),
            'revenue_trend': recent_orders['order_amount'].sum() / max(1, group['order_amount'].sum()) * 100
        }
        
        # Calculate health score (0-100)
        health_score = (
            min(25, metrics['recent_orders'] * 5) +  # Recent activity (max 25)
            min(25, metrics['order_frequency'] * 10) +  # Frequency (max 25)
            min(25, metrics['avg_order_value'] / 10) +  # Order value (max 25)
            max(0, 25 - metrics['days_since_last_order'] / 10)  # Recency (max 25)
        )
        
        metrics['health_score'] = health_score
        return pd.Series(metrics)
    
    # Apply custom aggregation
    customer_health = complete_data.groupby('customer_id').apply(customer_health_score)
    
    print("Customer Health Scores (Top 10 Healthiest):")
    print(customer_health.nlargest(10, 'health_score')[['health_score', 'total_revenue', 'order_frequency']].round(2))
    
    # Identify customers needing attention
    at_risk_customers = customer_health[
        (customer_health['health_score'] < 30) & 
        (customer_health['total_revenue'] > customer_health['total_revenue'].median())
    ]
    
    print(f"\nðŸš¨ High-Value Customers at Risk: {len(at_risk_customers)}")
    print("These customers need immediate attention!")
    
    return customer_health

health_scores = advanced_aggregations()
```

## ðŸŽ¯ Transformation Best Practices

### 1. **Performance Optimization**

```python
# âŒ Slow approach
def slow_transformation(df):
    results = []
    for customer_id in df['customer_id'].unique():
        customer_data = df[df['customer_id'] == customer_id]
        total_spent = customer_data['order_amount'].sum()
        results.append({'customer_id': customer_id, 'total_spent': total_spent})
    return pd.DataFrame(results)

# âœ… Fast approach
def fast_transformation(df):
    return df.groupby('customer_id')['order_amount'].sum().reset_index()

# Performance comparison
import time

start_time = time.time()
# fast_result = fast_transformation(complete_data)
end_time = time.time()

print(f"\nâš¡ PERFORMANCE OPTIMIZATION")
print(f"Fast groupby approach: {end_time - start_time:.4f} seconds")
print("Rule: Always use vectorized operations instead of loops!")
```

### 2. **Data Quality Validation**

```python
def validate_transformation_results(df, description="dataset"):
    """Validate data transformation results"""
    
    print(f"\nðŸ” DATA QUALITY CHECK: {description}")
    print("=" * 50)
    
    # Check for missing values
    missing_data = df.isnull().sum()
    if missing_data.any():
        print("âš ï¸ Missing Data Found:")
        print(missing_data[missing_data > 0])
    else:
        print("âœ… No missing data")
    
    # Check for duplicates
    duplicates = df.duplicated().sum()
    print(f"Duplicate rows: {duplicates}")
    
    # Check data types
    print("\nData Types:")
    print(df.dtypes)
    
    # Basic statistics
    print(f"\nDataset Shape: {df.shape}")
    print(f"Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
    
    return df

# Validate our transformations
validated_customer_analytics = validate_transformation_results(
    customer_analytics, 
    "Customer Analytics Results"
)
```

## ðŸŽ® Practice Challenges

### Challenge 1: Sales Performance Dashboard Data

```python
def sales_dashboard_challenge():
    """Prepare data for executive sales dashboard"""
    
    # Your mission: Transform raw orders into executive dashboard data
    # Required outputs:
    # 1. Monthly revenue trends
    # 2. Top performing regions
    # 3. Product category performance
    # 4. Customer segment analysis
    # 5. YoY growth rates
    
    print("ðŸŽ¯ SALES DASHBOARD CHALLENGE")
    print("Transform raw order data into executive insights!")
    
    # Starter code:
    dashboard_data = complete_data.copy()
    
    # TODO: Add your transformation logic here
    # Hint: Use groupby, pivot_table, and time series operations
    
    return dashboard_data

# Try the challenge!
# dashboard_ready_data = sales_dashboard_challenge()
```

### Challenge 2: Customer Lifetime Value Calculation

```python
def clv_calculation_challenge():
    """Calculate Customer Lifetime Value with complex business logic"""
    
    # Business requirements:
    # - Account for different product margins
    # - Factor in customer acquisition costs
    # - Consider customer tenure
    # - Predict future value based on trends
    
    print("ðŸ’° CLV CALCULATION CHALLENGE")
    print("Build a sophisticated CLV model!")
    
    # TODO: Implement CLV calculation
    # Hint: Combine multiple data sources and use custom aggregations
    
    pass

# clv_results = clv_calculation_challenge()
```

## ðŸŽ¯ Key Transformation Principles

1. **Start with business questions**: What decisions need to be made?
2. **Design for your audience**: Executives need summaries, analysts need details
3. **Validate at every step**: Check data quality after each transformation
4. **Optimize for performance**: Use vectorized operations, avoid loops
5. **Document your logic**: Future you will thank present you

## ðŸš€ What's Next?

You've mastered data transformation! Next up: **Time Series Mastery** - learn to work with dates, times, and temporal patterns that drive business cycles.

**Key skills unlocked:**
- âœ… Complex groupby operations
- âœ… Multi-source data integration
- âœ… Custom business logic implementation
- âœ… Performance-optimized transformations
- âœ… Data quality validation

Ready to tackle time-based data? Let's dive into **Time Series Mastery**! ðŸ“ˆ
