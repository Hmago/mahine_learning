{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9079f4cb",
   "metadata": {},
   "source": [
    "# Classification Algorithms: Hands-On Practice üéØ\n",
    "\n",
    "Welcome to your comprehensive classification workshop! In this notebook, you'll:\n",
    "\n",
    "- üîç **Explore real datasets** with different characteristics\n",
    "- üõ†Ô∏è **Implement multiple algorithms** from scratch and with scikit-learn\n",
    "- üìä **Compare performance** across different scenarios\n",
    "- üéÆ **Interactive exercises** to test your understanding\n",
    "- üèÜ **Build a complete classification pipeline**\n",
    "\n",
    "Let's dive in! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69eb3a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification, load_breast_cancer, load_wine\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üéØ Classification Workshop Setup Complete!\")\n",
    "print(\"Ready to explore the world of classification algorithms!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f170c3e",
   "metadata": {},
   "source": [
    "## Dataset 1: Breast Cancer Detection üè•\n",
    "\n",
    "Our first challenge: Build a model to detect malignant vs benign breast tumors.\n",
    "This is a **binary classification** problem with real-world medical implications!\n",
    "\n",
    "**Business Context:** \n",
    "- False Negative (missing cancer) = Very Bad üò∞\n",
    "- False Positive (false alarm) = Less bad but still concerning üòü"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c307df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the breast cancer dataset\n",
    "cancer_data = load_breast_cancer()\n",
    "X_cancer = pd.DataFrame(cancer_data.data, columns=cancer_data.feature_names)\n",
    "y_cancer = cancer_data.target\n",
    "\n",
    "print(\"üè• BREAST CANCER DATASET OVERVIEW\")\n",
    "print(f\"Samples: {X_cancer.shape[0]}\")\n",
    "print(f\"Features: {X_cancer.shape[1]}\")\n",
    "print(f\"Classes: {np.unique(y_cancer)} (0=Malignant, 1=Benign)\")\n",
    "print(f\"Class distribution: {np.bincount(y_cancer)}\")\n",
    "print(f\"Balance: {np.bincount(y_cancer)[0]/len(y_cancer):.1%} Malignant, {np.bincount(y_cancer)[1]/len(y_cancer):.1%} Benign\")\n",
    "\n",
    "# Display first few features\n",
    "print(\"\\nFirst 5 features:\")\n",
    "print(X_cancer.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3f2e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Feature distribution\n",
    "X_cancer['mean_radius'].hist(bins=30, alpha=0.7, ax=axes[0,0])\n",
    "axes[0,0].set_title('Mean Radius Distribution')\n",
    "axes[0,0].set_xlabel('Mean Radius')\n",
    "\n",
    "# Class distribution\n",
    "class_counts = pd.Series(y_cancer).value_counts()\n",
    "axes[0,1].bar(['Malignant', 'Benign'], class_counts.values, color=['red', 'green'], alpha=0.7)\n",
    "axes[0,1].set_title('Class Distribution')\n",
    "axes[0,1].set_ylabel('Count')\n",
    "\n",
    "# Feature correlation heatmap (subset)\n",
    "corr_subset = X_cancer[['mean_radius', 'mean_texture', 'mean_perimeter', 'mean_area', 'mean_smoothness']].corr()\n",
    "sns.heatmap(corr_subset, annot=True, cmap='coolwarm', ax=axes[1,0])\n",
    "axes[1,0].set_title('Feature Correlations (Subset)')\n",
    "\n",
    "# Boxplot: Malignant vs Benign for a key feature\n",
    "cancer_df = X_cancer.copy()\n",
    "cancer_df['target'] = y_cancer\n",
    "cancer_df['target_name'] = cancer_df['target'].map({0: 'Malignant', 1: 'Benign'})\n",
    "sns.boxplot(data=cancer_df, x='target_name', y='mean_radius', ax=axes[1,1])\n",
    "axes[1,1].set_title('Mean Radius by Diagnosis')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîç INSIGHTS:\")\n",
    "print(\"- Mean radius is clearly different between malignant and benign tumors\")\n",
    "print(\"- Some features are highly correlated (radius, perimeter, area)\")\n",
    "print(\"- Dataset is reasonably balanced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855718fc",
   "metadata": {},
   "source": [
    "### üéÆ Interactive Exercise 1: Your First Classification Model\n",
    "\n",
    "**Challenge:** Build a logistic regression model to predict cancer diagnosis.\n",
    "\n",
    "**Your Task:**\n",
    "1. Split the data (80% train, 20% test)\n",
    "2. Scale the features (cancer features have very different ranges!)\n",
    "3. Train a logistic regression model\n",
    "4. Evaluate using multiple metrics\n",
    "5. Interpret the results\n",
    "\n",
    "**Think about:** What metrics matter most for cancer detection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beace422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here! \n",
    "# Hint: Start by splitting the data using train_test_split\n",
    "\n",
    "# Step 1: Split the data\n",
    "X_train_cancer, X_test_cancer, y_train_cancer, y_test_cancer = train_test_split(\n",
    "    X_cancer, y_cancer, test_size=0.2, random_state=42, stratify=y_cancer\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Data split complete!\")\n",
    "print(f\"Training set: {X_train_cancer.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test_cancer.shape[0]} samples\")\n",
    "\n",
    "# Check if you maintained class balance\n",
    "print(f\"Training class distribution: {np.bincount(y_train_cancer)}\")\n",
    "print(f\"Test class distribution: {np.bincount(y_test_cancer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed79af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_cancer_scaled = scaler.fit_transform(X_train_cancer)\n",
    "X_test_cancer_scaled = scaler.transform(X_test_cancer)\n",
    "\n",
    "print(\"‚úÖ Feature scaling complete!\")\n",
    "print(f\"Original feature range (mean radius): {X_train_cancer['mean_radius'].min():.2f} to {X_train_cancer['mean_radius'].max():.2f}\")\n",
    "print(f\"Scaled feature range (mean radius): {X_train_cancer_scaled[:, 0].min():.2f} to {X_train_cancer_scaled[:, 0].max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be81e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Train logistic regression\n",
    "lr_cancer = LogisticRegression(random_state=42)\n",
    "lr_cancer.fit(X_train_cancer_scaled, y_train_cancer)\n",
    "\n",
    "# Step 4: Make predictions\n",
    "y_pred_cancer = lr_cancer.predict(X_test_cancer_scaled)\n",
    "y_proba_cancer = lr_cancer.predict_proba(X_test_cancer_scaled)[:, 1]\n",
    "\n",
    "print(\"‚úÖ Model training complete!\")\n",
    "print(f\"Model accuracy: {lr_cancer.score(X_test_cancer_scaled, y_test_cancer):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ffb07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Comprehensive evaluation\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_model(y_true, y_pred, y_proba, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation function\n",
    "    \"\"\"\n",
    "    accuracy = (y_true == y_pred).mean()\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_proba)\n",
    "    \n",
    "    print(f\"üìä {model_name.upper()} EVALUATION RESULTS:\")\n",
    "    print(f\"Accuracy:  {accuracy:.3f} - Overall correctness\")\n",
    "    print(f\"Precision: {precision:.3f} - When we predict benign, how often are we right?\")\n",
    "    print(f\"Recall:    {recall:.3f} - Of all actual benign cases, how many did we catch?\")\n",
    "    print(f\"F1-Score:  {f1:.3f} - Balance between precision and recall\")\n",
    "    print(f\"AUC:       {auc:.3f} - Overall discriminative ability\")\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Pred: Malignant', 'Pred: Benign'],\n",
    "                yticklabels=['True: Malignant', 'True: Benign'])\n",
    "    plt.title(f'{model_name} Confusion Matrix')\n",
    "    \n",
    "    # ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=f'{model_name} (AUC = {auc:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random Classifier')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Medical interpretation\n",
    "    print(f\"\\nüè• MEDICAL INTERPRETATION:\")\n",
    "    print(f\"False Negatives (missed cancer): {cm[0,0]} cases\")\n",
    "    print(f\"False Positives (unnecessary worry): {cm[1,0]} cases\")\n",
    "    print(f\"True Positives (correctly identified benign): {cm[1,1]} cases\")\n",
    "    print(f\"True Negatives (correctly identified malignant): {cm[0,1]} cases\")\n",
    "    \n",
    "    return accuracy, precision, recall, f1, auc\n",
    "\n",
    "# Evaluate our logistic regression model\n",
    "lr_results = evaluate_model(y_test_cancer, y_pred_cancer, y_proba_cancer, \"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577bf2ed",
   "metadata": {},
   "source": [
    "### üèÜ Algorithm Comparison Arena\n",
    "\n",
    "Now let's compare multiple algorithms on the same dataset! \n",
    "This is where the fun begins - seeing how different approaches handle the same problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76506de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define multiple algorithms to compare\n",
    "algorithms = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(probability=True, random_state=42),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Naive Bayes': GaussianNB()\n",
    "}\n",
    "\n",
    "# Store results\n",
    "comparison_results = {}\n",
    "\n",
    "print(\"üèüÔ∏è ALGORITHM COMPARISON ARENA\")\n",
    "print(\"Training and evaluating multiple algorithms...\")\n",
    "\n",
    "for name, algorithm in algorithms.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train the algorithm\n",
    "    if name in ['Logistic Regression', 'SVM', 'K-Nearest Neighbors']:\n",
    "        # These algorithms benefit from scaling\n",
    "        algorithm.fit(X_train_cancer_scaled, y_train_cancer)\n",
    "        y_pred = algorithm.predict(X_test_cancer_scaled)\n",
    "        y_proba = algorithm.predict_proba(X_test_cancer_scaled)[:, 1]\n",
    "    else:\n",
    "        # Tree-based algorithms don't need scaling\n",
    "        algorithm.fit(X_train_cancer, y_train_cancer)\n",
    "        y_pred = algorithm.predict(X_test_cancer)\n",
    "        y_proba = algorithm.predict_proba(X_test_cancer)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = (y_test_cancer == y_pred).mean()\n",
    "    precision = precision_score(y_test_cancer, y_pred)\n",
    "    recall = recall_score(y_test_cancer, y_pred)\n",
    "    f1 = f1_score(y_test_cancer, y_pred)\n",
    "    auc = roc_auc_score(y_test_cancer, y_proba)\n",
    "    \n",
    "    comparison_results[name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'AUC': auc\n",
    "    }\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame(comparison_results).T\n",
    "print(\"\\nüìä FINAL RESULTS:\")\n",
    "print(comparison_df.round(3))\n",
    "\n",
    "# Visualize comparison\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']\n",
    "for i, metric in enumerate(metrics):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    values = comparison_df[metric]\n",
    "    bars = plt.bar(range(len(values)), values, alpha=0.7)\n",
    "    plt.xticks(range(len(values)), values.index, rotation=45, ha='right')\n",
    "    plt.ylabel(metric)\n",
    "    plt.title(f'{metric} Comparison')\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Highlight best performer\n",
    "    best_idx = values.idxmax()\n",
    "    best_bar_idx = values.index.get_loc(best_idx)\n",
    "    bars[best_bar_idx].set_color('gold')\n",
    "    bars[best_bar_idx].set_edgecolor('black')\n",
    "    bars[best_bar_idx].set_linewidth(2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find overall best performer\n",
    "overall_scores = comparison_df.mean(axis=1).sort_values(ascending=False)\n",
    "print(f\"\\nüèÜ CHAMPION: {overall_scores.index[0]}\")\n",
    "print(f\"Average score across all metrics: {overall_scores.iloc[0]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac15c48",
   "metadata": {},
   "source": [
    "## Dataset 2: Wine Classification üç∑\n",
    "\n",
    "Let's tackle a **multiclass classification** problem! \n",
    "Predicting wine type based on chemical properties.\n",
    "\n",
    "**Business Context:** \n",
    "- Wine quality control and authentication\n",
    "- 3 different wine classes to predict\n",
    "- All mistakes are roughly equal cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40919877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load wine dataset\n",
    "wine_data = load_wine()\n",
    "X_wine = pd.DataFrame(wine_data.data, columns=wine_data.feature_names)\n",
    "y_wine = wine_data.target\n",
    "\n",
    "print(\"üç∑ WINE CLASSIFICATION DATASET\")\n",
    "print(f\"Samples: {X_wine.shape[0]}\")\n",
    "print(f\"Features: {X_wine.shape[1]}\")\n",
    "print(f\"Classes: {np.unique(y_wine)} - {wine_data.target_names}\")\n",
    "print(f\"Class distribution: {np.bincount(y_wine)}\")\n",
    "\n",
    "# Quick visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Feature distributions by class\n",
    "for class_idx, class_name in enumerate(wine_data.target_names):\n",
    "    class_data = X_wine[y_wine == class_idx]['alcohol']\n",
    "    axes[0].hist(class_data, alpha=0.7, label=f'Class {class_idx}: {class_name}', bins=15)\n",
    "\n",
    "axes[0].set_xlabel('Alcohol Content')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Alcohol Content by Wine Class')\n",
    "axes[0].legend()\n",
    "\n",
    "# 2D scatter plot\n",
    "scatter = axes[1].scatter(X_wine['alcohol'], X_wine['malic_acid'], c=y_wine, cmap='viridis', alpha=0.7)\n",
    "axes[1].set_xlabel('Alcohol Content')\n",
    "axes[1].set_ylabel('Malic Acid')\n",
    "axes[1].set_title('Wine Classes in 2D')\n",
    "plt.colorbar(scatter, ax=axes[1])\n",
    "\n",
    "# Class distribution\n",
    "class_counts = np.bincount(y_wine)\n",
    "axes[2].bar(wine_data.target_names, class_counts, alpha=0.7, color=['red', 'green', 'blue'])\n",
    "axes[2].set_title('Class Distribution')\n",
    "axes[2].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190553d3",
   "metadata": {},
   "source": [
    "### üéÆ Interactive Exercise 2: Multiclass Classification\n",
    "\n",
    "**Challenge:** Build a complete pipeline for wine classification!\n",
    "\n",
    "**Your Mission:**\n",
    "1. Split and scale the data\n",
    "2. Train multiple algorithms\n",
    "3. Use multiclass evaluation metrics\n",
    "4. Find the best performer\n",
    "5. Analyze feature importance\n",
    "\n",
    "**New Twist:** This time, you'll implement cross-validation for more robust results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f40234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement multiclass classification pipeline\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# Step 1: Split the data\n",
    "X_train_wine, X_test_wine, y_train_wine, y_test_wine = train_test_split(\n",
    "    X_wine, y_wine, test_size=0.2, random_state=42, stratify=y_wine\n",
    ")\n",
    "\n",
    "# Step 2: Scale features\n",
    "scaler_wine = StandardScaler()\n",
    "X_train_wine_scaled = scaler_wine.fit_transform(X_train_wine)\n",
    "X_test_wine_scaled = scaler_wine.transform(X_test_wine)\n",
    "\n",
    "print(\"‚úÖ Wine data prepared!\")\n",
    "print(f\"Training set shape: {X_train_wine_scaled.shape}\")\n",
    "print(f\"Test set shape: {X_test_wine_scaled.shape}\")\n",
    "\n",
    "# Step 3: Cross-validation comparison\n",
    "wine_algorithms = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(probability=True, random_state=42),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5)\n",
    "}\n",
    "\n",
    "wine_cv_results = {}\n",
    "\n",
    "print(\"\\nüç∑ WINE CLASSIFICATION - CROSS VALIDATION RESULTS\")\n",
    "for name, algorithm in wine_algorithms.items():\n",
    "    print(f\"Cross-validating {name}...\")\n",
    "    \n",
    "    # Use appropriate features (scaled for some algorithms)\n",
    "    if name in ['Logistic Regression', 'SVM', 'K-Nearest Neighbors']:\n",
    "        X_cv = X_train_wine_scaled\n",
    "    else:\n",
    "        X_cv = X_train_wine\n",
    "    \n",
    "    # 5-fold cross-validation with multiple metrics\n",
    "    cv_results = cross_validate(\n",
    "        algorithm, X_cv, y_train_wine, \n",
    "        cv=5, \n",
    "        scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'],\n",
    "        return_train_score=True\n",
    "    )\n",
    "    \n",
    "    wine_cv_results[name] = {\n",
    "        'CV_Accuracy': cv_results['test_accuracy'].mean(),\n",
    "        'CV_Precision': cv_results['test_precision_macro'].mean(),\n",
    "        'CV_Recall': cv_results['test_recall_macro'].mean(),\n",
    "        'CV_F1': cv_results['test_f1_macro'].mean(),\n",
    "        'CV_Std': cv_results['test_accuracy'].std()\n",
    "    }\n",
    "\n",
    "wine_cv_df = pd.DataFrame(wine_cv_results).T\n",
    "print(\"\\nüìä CROSS-VALIDATION RESULTS:\")\n",
    "print(wine_cv_df.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5723e5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train best model on full training set and evaluate on test set\n",
    "best_wine_model = wine_cv_df['CV_F1'].idxmax()\n",
    "print(f\"üèÜ Best model: {best_wine_model}\")\n",
    "\n",
    "# Train and evaluate the best model\n",
    "if best_wine_model in ['Logistic Regression', 'SVM', 'K-Nearest Neighbors']:\n",
    "    best_algorithm = wine_algorithms[best_wine_model]\n",
    "    best_algorithm.fit(X_train_wine_scaled, y_train_wine)\n",
    "    y_pred_wine = best_algorithm.predict(X_test_wine_scaled)\n",
    "    y_proba_wine = best_algorithm.predict_proba(X_test_wine_scaled)\n",
    "else:\n",
    "    best_algorithm = wine_algorithms[best_wine_model]\n",
    "    best_algorithm.fit(X_train_wine, y_train_wine)\n",
    "    y_pred_wine = best_algorithm.predict(X_test_wine)\n",
    "    y_proba_wine = best_algorithm.predict_proba(X_test_wine)\n",
    "\n",
    "# Multiclass evaluation\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(f\"\\nüìã DETAILED CLASSIFICATION REPORT - {best_wine_model}\")\n",
    "print(classification_report(y_test_wine, y_pred_wine, target_names=wine_data.target_names))\n",
    "\n",
    "# Multiclass confusion matrix\n",
    "cm_wine = confusion_matrix(y_test_wine, y_pred_wine)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(cm_wine, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=wine_data.target_names,\n",
    "            yticklabels=wine_data.target_names)\n",
    "plt.title(f'{best_wine_model} - Confusion Matrix')\n",
    "plt.ylabel('True Class')\n",
    "plt.xlabel('Predicted Class')\n",
    "\n",
    "# Feature importance (if available)\n",
    "if hasattr(best_algorithm, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': wine_data.feature_names,\n",
    "        'Importance': best_algorithm.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False).head(10)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.barh(range(len(feature_importance)), feature_importance['Importance'])\n",
    "    plt.yticks(range(len(feature_importance)), feature_importance['Feature'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title('Top 10 Most Important Features')\n",
    "elif hasattr(best_algorithm, 'coef_'):\n",
    "    # For linear models, show coefficient magnitudes\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': wine_data.feature_names,\n",
    "        'Importance': np.abs(best_algorithm.coef_).mean(axis=0)\n",
    "    }).sort_values('Importance', ascending=False).head(10)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.barh(range(len(feature_importance)), feature_importance['Importance'])\n",
    "    plt.yticks(range(len(feature_importance)), feature_importance['Feature'])\n",
    "    plt.xlabel('Average |Coefficient|')\n",
    "    plt.title('Top 10 Most Important Features')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüéØ KEY INSIGHTS:\")\n",
    "print(f\"- {best_wine_model} achieved {(y_pred_wine == y_test_wine).mean():.1%} accuracy on test set\")\n",
    "print(f\"- Cross-validation accuracy: {wine_cv_results[best_wine_model]['CV_Accuracy']:.1%} ¬± {wine_cv_results[best_wine_model]['CV_Std']:.1%}\")\n",
    "print(f\"- All three wine classes can be distinguished quite well!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9960ba7c",
   "metadata": {},
   "source": [
    "## Dataset 3: Custom Synthetic Dataset üé®\n",
    "\n",
    "Let's create our own dataset with specific characteristics to test algorithm behavior!\n",
    "This is great for understanding when each algorithm shines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46de0f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create different types of synthetic datasets\n",
    "def create_challenge_datasets():\n",
    "    \"\"\"\n",
    "    Create datasets with different characteristics to challenge our algorithms\n",
    "    \"\"\"\n",
    "    datasets = {}\n",
    "    \n",
    "    # Dataset 1: Linearly separable\n",
    "    X_linear, y_linear = make_classification(\n",
    "        n_samples=1000, n_features=2, n_redundant=0, n_informative=2,\n",
    "        n_clusters_per_class=1, class_sep=2, random_state=42\n",
    "    )\n",
    "    datasets['Linear'] = (X_linear, y_linear, \"Linearly separable classes\")\n",
    "    \n",
    "    # Dataset 2: Non-linear (XOR-like)\n",
    "    X_nonlinear, y_nonlinear = make_classification(\n",
    "        n_samples=1000, n_features=2, n_redundant=0, n_informative=2,\n",
    "        n_clusters_per_class=2, class_sep=0.5, random_state=42\n",
    "    )\n",
    "    datasets['Non-Linear'] = (X_nonlinear, y_nonlinear, \"Non-linearly separable (multiple clusters per class)\")\n",
    "    \n",
    "    # Dataset 3: High noise\n",
    "    X_noisy, y_noisy = make_classification(\n",
    "        n_samples=1000, n_features=2, n_redundant=0, n_informative=2,\n",
    "        n_clusters_per_class=1, class_sep=1, flip_y=0.15, random_state=42\n",
    "    )\n",
    "    datasets['Noisy'] = (X_noisy, y_noisy, \"High noise (15% label flip)\")\n",
    "    \n",
    "    # Dataset 4: High dimensional\n",
    "    X_highdim, y_highdim = make_classification(\n",
    "        n_samples=500, n_features=100, n_informative=20, n_redundant=80,\n",
    "        n_clusters_per_class=1, random_state=42\n",
    "    )\n",
    "    datasets['High-Dim'] = (X_highdim, y_highdim, \"High dimensional (100 features)\")\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "challenge_datasets = create_challenge_datasets()\n",
    "\n",
    "# Visualize the 2D datasets\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for i, (name, (X, y, description)) in enumerate(list(challenge_datasets.items())[:3]):\n",
    "    if X.shape[1] == 2:  # Only plot 2D datasets\n",
    "        scatter = axes[i].scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', alpha=0.7)\n",
    "        axes[i].set_title(f'{name} Dataset\\n{description}')\n",
    "        axes[i].set_xlabel('Feature 1')\n",
    "        axes[i].set_ylabel('Feature 2')\n",
    "        plt.colorbar(scatter, ax=axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üé® SYNTHETIC DATASETS CREATED!\")\n",
    "print(\"Each dataset tests different algorithm strengths:\")\n",
    "print(\"- Linear: Should favor linear models (Logistic Regression, SVM)\")\n",
    "print(\"- Non-Linear: Should favor flexible models (Trees, KNN)\")  \n",
    "print(\"- Noisy: Should favor robust models (Random Forest)\")\n",
    "print(\"- High-Dim: Should favor regularized models (Logistic Regression)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f692bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare algorithms across all challenge datasets\n",
    "challenge_algorithms = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42, max_depth=10),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=50, random_state=42),\n",
    "    'SVM (RBF)': SVC(kernel='rbf', random_state=42),\n",
    "    'K-NN (k=5)': KNeighborsClassifier(n_neighbors=5),\n",
    "}\n",
    "\n",
    "# Store all results\n",
    "all_results = {}\n",
    "\n",
    "print(\"üèüÔ∏è ALGORITHM CHALLENGE ARENA\")\n",
    "print(\"Testing each algorithm on different dataset types...\")\n",
    "\n",
    "for dataset_name, (X, y, description) in challenge_datasets.items():\n",
    "    print(f\"\\nüìä Testing on {dataset_name} dataset: {description}\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    dataset_results = {}\n",
    "    \n",
    "    for alg_name, algorithm in challenge_algorithms.items():\n",
    "        # Use scaled features for distance-based algorithms\n",
    "        if alg_name in ['Logistic Regression', 'SVM (RBF)', 'K-NN (k=5)']:\n",
    "            algorithm.fit(X_train_scaled, y_train)\n",
    "            accuracy = algorithm.score(X_test_scaled, y_test)\n",
    "        else:\n",
    "            algorithm.fit(X_train, y_train)\n",
    "            accuracy = algorithm.score(X_test, y_test)\n",
    "        \n",
    "        dataset_results[alg_name] = accuracy\n",
    "    \n",
    "    all_results[dataset_name] = dataset_results\n",
    "\n",
    "# Create comprehensive results DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "print(\"\\nüèÜ FINAL CHALLENGE RESULTS:\")\n",
    "print(results_df.round(3))\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(results_df, annot=True, cmap='RdYlGn', center=0.8, \n",
    "            fmt='.3f', cbar_kws={'label': 'Accuracy'})\n",
    "plt.title('Algorithm Performance Across Different Dataset Types')\n",
    "plt.ylabel('Algorithm')\n",
    "plt.xlabel('Dataset Type')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find best algorithm for each dataset type\n",
    "print(\"\\nü•á WINNERS FOR EACH DATASET TYPE:\")\n",
    "for dataset_name in results_df.columns:\n",
    "    best_alg = results_df[dataset_name].idxmax()\n",
    "    best_score = results_df[dataset_name].max()\n",
    "    print(f\"{dataset_name}: {best_alg} ({best_score:.3f})\")\n",
    "\n",
    "# Find most versatile algorithm\n",
    "versatility_scores = results_df.mean(axis=1).sort_values(ascending=False)\n",
    "print(f\"\\nüåü MOST VERSATILE ALGORITHM: {versatility_scores.index[0]}\")\n",
    "print(f\"Average performance: {versatility_scores.iloc[0]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21a97cd",
   "metadata": {},
   "source": [
    "### üéÆ Interactive Exercise 3: Hyperparameter Tuning\n",
    "\n",
    "**Final Challenge:** Optimize your best algorithm using Grid Search!\n",
    "\n",
    "**Your Mission:**\n",
    "1. Choose the best performing algorithm from above\n",
    "2. Define a hyperparameter grid to search\n",
    "3. Use GridSearchCV with cross-validation\n",
    "4. Compare default vs optimized performance\n",
    "5. Analyze which parameters matter most\n",
    "\n",
    "**Pro Tip:** Start with a coarse grid, then refine around the best values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4c3e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement hyperparameter tuning for your chosen algorithm\n",
    "\n",
    "# Choose Random Forest as it performed well across datasets\n",
    "print(\"üîß HYPERPARAMETER TUNING: Random Forest\")\n",
    "\n",
    "# Define parameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 10, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "print(f\"Parameter grid size: {np.prod([len(v) for v in param_grid.values()])} combinations\")\n",
    "\n",
    "# Use the breast cancer dataset for tuning (medical importance!)\n",
    "rf_tuning = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Perform grid search\n",
    "print(\"üîç Performing Grid Search (this might take a moment...)\")\n",
    "grid_search = GridSearchCV(\n",
    "    rf_tuning, \n",
    "    param_grid, \n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring='f1',  # Optimize for F1-score (good for medical data)\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_cancer_scaled, y_train_cancer)\n",
    "\n",
    "print(\"‚úÖ Grid Search Complete!\")\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation F1 score: {grid_search.best_score_:.3f}\")\n",
    "\n",
    "# Compare default vs optimized\n",
    "rf_default = RandomForestClassifier(random_state=42)\n",
    "rf_default.fit(X_train_cancer, y_train_cancer)\n",
    "\n",
    "rf_optimized = grid_search.best_estimator_\n",
    "# Note: GridSearch was done on scaled data, so let's retrain on unscaled for fair comparison\n",
    "rf_optimized.fit(X_train_cancer, y_train_cancer)\n",
    "\n",
    "# Test both models\n",
    "y_pred_default = rf_default.predict(X_test_cancer)\n",
    "y_pred_optimized = rf_optimized.predict(X_test_cancer)\n",
    "\n",
    "print(f\"\\nüìä DEFAULT vs OPTIMIZED COMPARISON:\")\n",
    "print(f\"Default Random Forest F1-Score: {f1_score(y_test_cancer, y_pred_default):.3f}\")\n",
    "print(f\"Optimized Random Forest F1-Score: {f1_score(y_test_cancer, y_pred_optimized):.3f}\")\n",
    "print(f\"Improvement: {f1_score(y_test_cancer, y_pred_optimized) - f1_score(y_test_cancer, y_pred_default):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b646662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze hyperparameter importance\n",
    "results_df_tuning = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Plot parameter importance\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "param_names = list(param_grid.keys())\n",
    "for i, param in enumerate(param_names):\n",
    "    if i < len(axes):\n",
    "        # Group by parameter and show mean scores\n",
    "        param_performance = results_df_tuning.groupby(f'param_{param}')['mean_test_score'].mean()\n",
    "        \n",
    "        axes[i].bar(range(len(param_performance)), param_performance.values, alpha=0.7)\n",
    "        axes[i].set_xticks(range(len(param_performance)))\n",
    "        axes[i].set_xticklabels([str(x) for x in param_performance.index], rotation=45)\n",
    "        axes[i].set_xlabel(param)\n",
    "        axes[i].set_ylabel('Mean CV F1 Score')\n",
    "        axes[i].set_title(f'Impact of {param}')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "# Feature importance from optimized model\n",
    "if len(axes) > len(param_names):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': X_cancer.columns,\n",
    "        'Importance': rf_optimized.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False).head(15)\n",
    "    \n",
    "    axes[-1].barh(range(len(feature_importance)), feature_importance['Importance'])\n",
    "    axes[-1].set_yticks(range(len(feature_importance)))\n",
    "    axes[-1].set_yticklabels(feature_importance['Feature'])\n",
    "    axes[-1].set_xlabel('Feature Importance')\n",
    "    axes[-1].set_title('Top 15 Features (Optimized Model)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üéØ TUNING INSIGHTS:\")\n",
    "print(\"- Look for parameters that show large performance differences\")\n",
    "print(\"- Feature importance can guide future feature engineering\")\n",
    "print(\"- Sometimes default parameters are already quite good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0223fe",
   "metadata": {},
   "source": [
    "## üèÜ Workshop Summary & Key Takeaways\n",
    "\n",
    "Congratulations! You've completed a comprehensive classification workshop. Let's summarize what you've learned:\n",
    "\n",
    "### üéØ Key Insights from Today:\n",
    "\n",
    "1. **Algorithm Selection Matters**: Different algorithms excel in different scenarios\n",
    "2. **Data Preprocessing is Crucial**: Scaling can dramatically affect performance\n",
    "3. **Evaluation Strategy**: Use multiple metrics, especially for medical/critical applications\n",
    "4. **Cross-Validation**: Provides more reliable performance estimates\n",
    "5. **Hyperparameter Tuning**: Can provide meaningful improvements\n",
    "6. **No Free Lunch**: No single algorithm is always best\n",
    "\n",
    "### üõ†Ô∏è Practical Skills Gained:\n",
    "\n",
    "- ‚úÖ Built and evaluated multiple classification models\n",
    "- ‚úÖ Handled both binary and multiclass problems\n",
    "- ‚úÖ Applied proper data preprocessing techniques\n",
    "- ‚úÖ Used cross-validation for robust evaluation\n",
    "- ‚úÖ Performed hyperparameter optimization\n",
    "- ‚úÖ Interpreted model results in business context\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "\n",
    "1. **Practice More**: Try these techniques on your own datasets\n",
    "2. **Advanced Topics**: Explore ensemble methods, deep learning\n",
    "3. **Production Skills**: Learn about model deployment and monitoring\n",
    "4. **Domain Expertise**: Apply to specific fields (healthcare, finance, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ece9c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéâ CLASSIFICATION WORKSHOP COMPLETE! üéâ\")\n",
    "print(\"\\nüìö What you've mastered today:\")\n",
    "print(\"‚úÖ Multiple classification algorithms\")\n",
    "print(\"‚úÖ Proper evaluation techniques\") \n",
    "print(\"‚úÖ Cross-validation and hyperparameter tuning\")\n",
    "print(\"‚úÖ Real-world problem solving\")\n",
    "print(\"\\nüöÄ You're ready to tackle classification problems in the wild!\")\n",
    "print(\"\\nKeep practicing and happy learning! ü§ñüìä\")\n",
    "\n",
    "# Final challenge for the ambitious learners\n",
    "print(\"\\nüèÜ BONUS CHALLENGE:\")\n",
    "print(\"Can you achieve >95% F1-score on the breast cancer dataset?\")\n",
    "print(\"Hint: Try ensemble methods, feature selection, or advanced preprocessing!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
