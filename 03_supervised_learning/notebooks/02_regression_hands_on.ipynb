{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45041786",
   "metadata": {},
   "source": [
    "# Regression Algorithms: From Linear to Advanced ðŸ“ˆ\n",
    "\n",
    "Welcome to your comprehensive regression workshop! Today you'll master:\n",
    "\n",
    "- ðŸ  **Real estate price prediction** with linear regression\n",
    "- ðŸ“Š **Advanced regression techniques** (Ridge, Lasso, Elastic Net)\n",
    "- ðŸŒŸ **Polynomial features** and non-linear relationships\n",
    "- ðŸ” **Feature selection** and regularization\n",
    "- ðŸ“ **Comprehensive evaluation** beyond just RÂ²\n",
    "- ðŸŽ¯ **End-to-end pipeline** for production-ready models\n",
    "\n",
    "Ready to predict the future? Let's go! ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462101e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports for regression analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_regression, load_diabetes\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, RFE\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"ðŸ“ˆ REGRESSION WORKSHOP INITIALIZED!\")\n",
    "print(\"Ready to predict continuous values with confidence!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a9a52c",
   "metadata": {},
   "source": [
    "## Dataset 1: California Housing Prices ðŸ \n",
    "\n",
    "Let's start with everyone's favorite: predicting house prices! \n",
    "This dataset contains real California housing data with features like:\n",
    "- Median income in the area\n",
    "- House age\n",
    "- Number of rooms\n",
    "- Population density\n",
    "- Geographic location\n",
    "\n",
    "**Business Goal:** Build a model to estimate house prices for real estate valuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7cb6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and create California housing dataset\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing_data = fetch_california_housing()\n",
    "X_housing = pd.DataFrame(housing_data.data, columns=housing_data.feature_names)\n",
    "y_housing = housing_data.target  # Prices in hundreds of thousands\n",
    "\n",
    "print(\"ðŸ  CALIFORNIA HOUSING DATASET\")\n",
    "print(f\"Samples: {X_housing.shape[0]:,}\")\n",
    "print(f\"Features: {X_housing.shape[1]}\")\n",
    "print(f\"Target: House price (in $100,000s)\")\n",
    "print(f\"Price range: ${y_housing.min():.1f}k - ${y_housing.max():.1f}k\")\n",
    "print(f\"Average price: ${y_housing.mean():.1f}k\")\n",
    "\n",
    "# Display feature information\n",
    "print(f\"\\nFeatures:\")\n",
    "for i, feature in enumerate(housing_data.feature_names):\n",
    "    print(f\"  {i+1}. {feature}\")\n",
    "\n",
    "print(f\"\\nFirst few samples:\")\n",
    "housing_df = X_housing.copy()\n",
    "housing_df['price'] = y_housing\n",
    "print(housing_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22028ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Exploratory Data Analysis\n",
    "fig, axes = plt.subplots(3, 3, figsize=(20, 15))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Distribution of target variable\n",
    "axes[0].hist(y_housing, bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[0].set_title('House Price Distribution')\n",
    "axes[0].set_xlabel('Price ($100k)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "# Feature distributions\n",
    "features_to_plot = ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population']\n",
    "for i, feature in enumerate(features_to_plot):\n",
    "    axes[i+1].hist(X_housing[feature], bins=30, alpha=0.7, edgecolor='black')\n",
    "    axes[i+1].set_title(f'{feature} Distribution')\n",
    "    axes[i+1].set_xlabel(feature)\n",
    "\n",
    "# Correlation with target\n",
    "correlations = X_housing.corrwith(pd.Series(y_housing, name='Price')).sort_values(ascending=False)\n",
    "axes[6].barh(range(len(correlations)), correlations.values, color='skyblue', edgecolor='black')\n",
    "axes[6].set_yticks(range(len(correlations)))\n",
    "axes[6].set_yticklabels(correlations.index)\n",
    "axes[6].set_xlabel('Correlation with Price')\n",
    "axes[6].set_title('Feature-Price Correlations')\n",
    "\n",
    "# Scatter plot: Income vs Price (strongest correlation)\n",
    "axes[7].scatter(X_housing['MedInc'], y_housing, alpha=0.5, s=1)\n",
    "axes[7].set_xlabel('Median Income')\n",
    "axes[7].set_ylabel('House Price ($100k)')\n",
    "axes[7].set_title('Income vs House Price')\n",
    "\n",
    "# Geographic scatter (Longitude vs Latitude colored by price)\n",
    "scatter = axes[8].scatter(X_housing['Longitude'], X_housing['Latitude'], \n",
    "                         c=y_housing, cmap='viridis', alpha=0.6, s=1)\n",
    "axes[8].set_xlabel('Longitude')\n",
    "axes[8].set_ylabel('Latitude') \n",
    "axes[8].set_title('Geographic Price Distribution')\n",
    "plt.colorbar(scatter, ax=axes[8], label='Price ($100k)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ” KEY INSIGHTS:\")\n",
    "print(f\"- Median Income has strongest correlation with price: {correlations['MedInc']:.3f}\")\n",
    "print(f\"- Geographic location clearly matters (coastal areas more expensive)\")\n",
    "print(f\"- Price distribution is right-skewed (some very expensive areas)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247c8894",
   "metadata": {},
   "source": [
    "### ðŸŽ® Interactive Exercise 1: Linear Regression Fundamentals\n",
    "\n",
    "**Your First Challenge:** Build a simple linear regression model to predict house prices.\n",
    "\n",
    "**Tasks:**\n",
    "1. Split the data (80% train, 20% test)\n",
    "2. Train a basic linear regression model  \n",
    "3. Evaluate using multiple regression metrics\n",
    "4. Visualize predictions vs actual values\n",
    "5. Analyze residuals (prediction errors)\n",
    "\n",
    "**Learning Goal:** Understand what linear regression can and cannot capture!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d88f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement linear regression pipeline\n",
    "\n",
    "# Step 1: Split the data\n",
    "X_train_housing, X_test_housing, y_train_housing, y_test_housing = train_test_split(\n",
    "    X_housing, y_housing, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"âœ… Data split complete!\")\n",
    "print(f\"Training samples: {X_train_housing.shape[0]:,}\")\n",
    "print(f\"Test samples: {X_test_housing.shape[0]:,}\")\n",
    "\n",
    "# Step 2: Train linear regression\n",
    "lr_housing = LinearRegression()\n",
    "lr_housing.fit(X_train_housing, y_train_housing)\n",
    "\n",
    "# Step 3: Make predictions\n",
    "y_pred_train = lr_housing.predict(X_train_housing)\n",
    "y_pred_test = lr_housing.predict(X_test_housing)\n",
    "\n",
    "print(\"âœ… Linear regression model trained!\")\n",
    "\n",
    "# Step 4: Comprehensive evaluation\n",
    "def evaluate_regression_model(y_true, y_pred, dataset_name=\"Dataset\"):\n",
    "    \"\"\"\n",
    "    Comprehensive regression model evaluation\n",
    "    \"\"\"\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š {dataset_name.upper()} EVALUATION:\")\n",
    "    print(f\"RÂ² Score:     {r2:.3f} - Variance explained\")\n",
    "    print(f\"RMSE:         ${rmse:.1f}k - Root Mean Squared Error\")\n",
    "    print(f\"MAE:          ${mae:.1f}k - Mean Absolute Error\")\n",
    "    print(f\"MSE:          ${mse:.1f} - Mean Squared Error\")\n",
    "    \n",
    "    # Business interpretation\n",
    "    print(f\"\\nðŸ’° BUSINESS INTERPRETATION:\")\n",
    "    print(f\"- On average, predictions are off by ${rmse:.1f}k\")\n",
    "    print(f\"- Model explains {r2:.1%} of price variation\")\n",
    "    print(f\"- Typical error magnitude: ${mae:.1f}k\")\n",
    "    \n",
    "    return {'R2': r2, 'RMSE': rmse, 'MAE': mae, 'MSE': mse}\n",
    "\n",
    "# Evaluate on both training and test sets\n",
    "train_metrics = evaluate_regression_model(y_train_housing, y_pred_train, \"Training\")\n",
    "test_metrics = evaluate_regression_model(y_test_housing, y_pred_test, \"Test\")\n",
    "\n",
    "# Check for overfitting\n",
    "print(f\"\\nðŸ” OVERFITTING CHECK:\")\n",
    "print(f\"Training RÂ²: {train_metrics['R2']:.3f}\")\n",
    "print(f\"Test RÂ²:     {test_metrics['R2']:.3f}\")\n",
    "print(f\"Difference:  {train_metrics['R2'] - test_metrics['R2']:.3f}\")\n",
    "\n",
    "if train_metrics['R2'] - test_metrics['R2'] > 0.1:\n",
    "    print(\"âš ï¸  Possible overfitting detected!\")\n",
    "else:\n",
    "    print(\"âœ… No significant overfitting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489b1c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Visualize results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Predictions vs Actual\n",
    "axes[0,0].scatter(y_test_housing, y_pred_test, alpha=0.6, s=20)\n",
    "axes[0,0].plot([y_test_housing.min(), y_test_housing.max()], \n",
    "               [y_test_housing.min(), y_test_housing.max()], 'r--', lw=2)\n",
    "axes[0,0].set_xlabel('Actual Price ($100k)')\n",
    "axes[0,0].set_ylabel('Predicted Price ($100k)')\n",
    "axes[0,0].set_title('Predictions vs Actual Values')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals (errors)\n",
    "residuals = y_test_housing - y_pred_test\n",
    "axes[0,1].scatter(y_pred_test, residuals, alpha=0.6, s=20)\n",
    "axes[0,1].axhline(y=0, color='r', linestyle='--')\n",
    "axes[0,1].set_xlabel('Predicted Price ($100k)')\n",
    "axes[0,1].set_ylabel('Residuals')\n",
    "axes[0,1].set_title('Residual Plot')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals histogram\n",
    "axes[1,0].hist(residuals, bins=30, alpha=0.7, edgecolor='black')\n",
    "axes[1,0].set_xlabel('Residuals')\n",
    "axes[1,0].set_ylabel('Frequency')\n",
    "axes[1,0].set_title('Residuals Distribution')\n",
    "axes[1,0].axvline(x=0, color='r', linestyle='--')\n",
    "\n",
    "# Feature coefficients\n",
    "coefficients = pd.DataFrame({\n",
    "    'Feature': X_housing.columns,\n",
    "    'Coefficient': lr_housing.coef_\n",
    "}).sort_values('Coefficient', key=abs, ascending=False)\n",
    "\n",
    "axes[1,1].barh(range(len(coefficients)), coefficients['Coefficient'])\n",
    "axes[1,1].set_yticks(range(len(coefficients)))\n",
    "axes[1,1].set_yticklabels(coefficients['Feature'])\n",
    "axes[1,1].set_xlabel('Coefficient Value')\n",
    "axes[1,1].set_title('Feature Importance (Coefficients)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸŽ¯ COEFFICIENT INTERPRETATION:\")\n",
    "for _, row in coefficients.head().iterrows():\n",
    "    direction = \"increases\" if row['Coefficient'] > 0 else \"decreases\"\n",
    "    print(f\"- {row['Feature']}: {direction} price by ${abs(row['Coefficient']):.1f}k per unit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f71545",
   "metadata": {},
   "source": [
    "## Advanced Regression: Regularization Techniques ðŸŽšï¸\n",
    "\n",
    "Linear regression can overfit, especially with many features. Let's explore regularization techniques:\n",
    "\n",
    "- **Ridge Regression (L2)**: Shrinks coefficients smoothly\n",
    "- **Lasso Regression (L1)**: Can set coefficients to exactly zero (feature selection)\n",
    "- **Elastic Net**: Combines both L1 and L2 penalties\n",
    "\n",
    "These techniques help prevent overfitting and can improve generalization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba87283a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different regularization techniques\n",
    "regularization_models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge (Î±=1.0)': Ridge(alpha=1.0),\n",
    "    'Ridge (Î±=10.0)': Ridge(alpha=10.0), \n",
    "    'Ridge (Î±=100.0)': Ridge(alpha=100.0),\n",
    "    'Lasso (Î±=0.1)': Lasso(alpha=0.1, max_iter=2000),\n",
    "    'Lasso (Î±=1.0)': Lasso(alpha=1.0, max_iter=2000),\n",
    "    'ElasticNet (Î±=1.0)': ElasticNet(alpha=1.0, l1_ratio=0.5, max_iter=2000)\n",
    "}\n",
    "\n",
    "# Need to scale features for regularized models\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_housing)\n",
    "X_test_scaled = scaler.transform(X_test_housing)\n",
    "\n",
    "regularization_results = {}\n",
    "\n",
    "print(\"ðŸŽšï¸ REGULARIZATION TECHNIQUES COMPARISON\")\n",
    "for name, model in regularization_models.items():\n",
    "    if 'Linear' in name:\n",
    "        # Don't scale for basic linear regression for comparison\n",
    "        model.fit(X_train_housing, y_train_housing)\n",
    "        y_pred = model.predict(X_test_housing)\n",
    "    else:\n",
    "        # Use scaled features for regularized models\n",
    "        model.fit(X_train_scaled, y_train_housing)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    r2 = r2_score(y_test_housing, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_housing, y_pred))\n",
    "    mae = mean_absolute_error(y_test_housing, y_pred)\n",
    "    \n",
    "    # Count non-zero coefficients (for feature selection analysis)\n",
    "    n_features_used = np.sum(np.abs(model.coef_) > 1e-4) if hasattr(model, 'coef_') else len(X_housing.columns)\n",
    "    \n",
    "    regularization_results[name] = {\n",
    "        'RÂ²': r2,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'Features Used': n_features_used\n",
    "    }\n",
    "\n",
    "# Display results\n",
    "reg_df = pd.DataFrame(regularization_results).T\n",
    "print(\"\\nðŸ“Š REGULARIZATION RESULTS:\")\n",
    "print(reg_df.round(3))\n",
    "\n",
    "# Visualize regularization effects\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Performance comparison\n",
    "metrics = ['RÂ²', 'RMSE', 'MAE']\n",
    "for i, metric in enumerate(metrics):\n",
    "    if i < 3:\n",
    "        ax_idx = (i//2, i%2) if i < 2 else (1, 0)\n",
    "        values = reg_df[metric]\n",
    "        bars = axes[ax_idx].bar(range(len(values)), values, alpha=0.7)\n",
    "        axes[ax_idx].set_xticks(range(len(values)))\n",
    "        axes[ax_idx].set_xticklabels(values.index, rotation=45, ha='right')\n",
    "        axes[ax_idx].set_ylabel(metric)\n",
    "        axes[ax_idx].set_title(f'{metric} Comparison')\n",
    "        axes[ax_idx].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Highlight best performer\n",
    "        if metric == 'RÂ²':\n",
    "            best_idx = values.idxmax()\n",
    "        else:\n",
    "            best_idx = values.idxmin()\n",
    "        best_bar_idx = values.index.get_loc(best_idx)\n",
    "        bars[best_bar_idx].set_color('gold')\n",
    "\n",
    "# Feature usage\n",
    "axes[1,1].bar(range(len(reg_df)), reg_df['Features Used'], alpha=0.7, color='lightcoral')\n",
    "axes[1,1].set_xticks(range(len(reg_df)))\n",
    "axes[1,1].set_xticklabels(reg_df.index, rotation=45, ha='right')\n",
    "axes[1,1].set_ylabel('Number of Features Used')\n",
    "axes[1,1].set_title('Feature Selection Effect')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Best performing model\n",
    "best_model = reg_df['RÂ²'].idxmax()\n",
    "print(f\"\\nðŸ† BEST PERFORMING MODEL: {best_model}\")\n",
    "print(f\"RÂ² Score: {reg_df.loc[best_model, 'RÂ²']:.3f}\")\n",
    "print(f\"Uses {reg_df.loc[best_model, 'Features Used']:.0f} out of {len(X_housing.columns)} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b96a48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze coefficient paths for different regularization strengths\n",
    "alphas = np.logspace(-3, 2, 50)  # From 0.001 to 100\n",
    "\n",
    "ridge_coefs = []\n",
    "lasso_coefs = []\n",
    "\n",
    "print(\"ðŸ” Analyzing regularization paths...\")\n",
    "\n",
    "for alpha in alphas:\n",
    "    # Ridge coefficients\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X_train_scaled, y_train_housing)\n",
    "    ridge_coefs.append(ridge.coef_)\n",
    "    \n",
    "    # Lasso coefficients  \n",
    "    lasso = Lasso(alpha=alpha, max_iter=2000)\n",
    "    lasso.fit(X_train_scaled, y_train_housing)\n",
    "    lasso_coefs.append(lasso.coef_)\n",
    "\n",
    "ridge_coefs = np.array(ridge_coefs)\n",
    "lasso_coefs = np.array(lasso_coefs)\n",
    "\n",
    "# Plot coefficient paths\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Ridge path\n",
    "for i in range(ridge_coefs.shape[1]):\n",
    "    axes[0].plot(alphas, ridge_coefs[:, i], label=X_housing.columns[i])\n",
    "axes[0].set_xscale('log')\n",
    "axes[0].set_xlabel('Alpha (Regularization Strength)')\n",
    "axes[0].set_ylabel('Coefficient Value')\n",
    "axes[0].set_title('Ridge Regression: Coefficient Paths')\n",
    "axes[0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Lasso path\n",
    "for i in range(lasso_coefs.shape[1]):\n",
    "    axes[1].plot(alphas, lasso_coefs[:, i], label=X_housing.columns[i])\n",
    "axes[1].set_xscale('log')\n",
    "axes[1].set_xlabel('Alpha (Regularization Strength)')\n",
    "axes[1].set_ylabel('Coefficient Value')\n",
    "axes[1].set_title('Lasso Regression: Coefficient Paths')\n",
    "axes[1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ“ˆ COEFFICIENT PATH INSIGHTS:\")\n",
    "print(\"- Ridge: Coefficients shrink smoothly but never reach exactly zero\")\n",
    "print(\"- Lasso: Coefficients can become exactly zero (automatic feature selection)\")\n",
    "print(\"- Higher alpha = more regularization = smaller coefficients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8270b88c",
   "metadata": {},
   "source": [
    "### ðŸŽ® Interactive Exercise 2: Hyperparameter Tuning with Cross-Validation\n",
    "\n",
    "**Challenge:** Find the optimal regularization strength using GridSearchCV!\n",
    "\n",
    "**Your Mission:**\n",
    "1. Define parameter grids for Ridge and Lasso\n",
    "2. Use cross-validation to find best parameters\n",
    "3. Compare optimized models\n",
    "4. Analyze the trade-off between bias and variance\n",
    "\n",
    "**Pro Tip:** Use learning curves to understand if you need more data or a different model complexity!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0db7c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement hyperparameter tuning\n",
    "\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "print(\"ðŸŽ¯ HYPERPARAMETER TUNING WITH CROSS-VALIDATION\")\n",
    "\n",
    "# Define parameter grids\n",
    "ridge_params = {'alpha': np.logspace(-2, 2, 20)}  # 0.01 to 100\n",
    "lasso_params = {'alpha': np.logspace(-3, 1, 20)}  # 0.001 to 10\n",
    "\n",
    "# Models to tune\n",
    "models_to_tune = {\n",
    "    'Ridge': (Ridge(), ridge_params),\n",
    "    'Lasso': (Lasso(max_iter=2000), lasso_params)\n",
    "}\n",
    "\n",
    "tuning_results = {}\n",
    "\n",
    "for model_name, (model, param_grid) in models_to_tune.items():\n",
    "    print(f\"\\nTuning {model_name}...\")\n",
    "    \n",
    "    # Grid search with cross-validation\n",
    "    grid_search = GridSearchCV(\n",
    "        model, param_grid, cv=5, scoring='r2', n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train_scaled, y_train_housing)\n",
    "    \n",
    "    # Store results\n",
    "    tuning_results[model_name] = {\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'best_cv_score': grid_search.best_score_,\n",
    "        'best_estimator': grid_search.best_estimator_\n",
    "    }\n",
    "    \n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best CV RÂ² score: {grid_search.best_score_:.3f}\")\n",
    "\n",
    "# Test optimized models on test set\n",
    "print(f\"\\nðŸ† OPTIMIZED MODELS TEST PERFORMANCE:\")\n",
    "\n",
    "for model_name, results in tuning_results.items():\n",
    "    best_model = results['best_estimator']\n",
    "    y_pred_optimized = best_model.predict(X_test_scaled)\n",
    "    test_r2 = r2_score(y_test_housing, y_pred_optimized)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test_housing, y_pred_optimized))\n",
    "    \n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  CV RÂ² Score: {results['best_cv_score']:.3f}\")\n",
    "    print(f\"  Test RÂ² Score: {test_r2:.3f}\")\n",
    "    print(f\"  Test RMSE: ${test_rmse:.1f}k\")\n",
    "    print(f\"  Generalization gap: {results['best_cv_score'] - test_r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f835d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create validation curves to understand the bias-variance tradeoff\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "for i, (model_name, (model, param_grid)) in enumerate(models_to_tune.items()):\n",
    "    param_name = list(param_grid.keys())[0]\n",
    "    param_range = param_grid[param_name]\n",
    "    \n",
    "    train_scores, val_scores = validation_curve(\n",
    "        model, X_train_scaled, y_train_housing,\n",
    "        param_name=param_name, param_range=param_range,\n",
    "        cv=5, scoring='r2', n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    train_mean = train_scores.mean(axis=1)\n",
    "    train_std = train_scores.std(axis=1)\n",
    "    val_mean = val_scores.mean(axis=1)\n",
    "    val_std = val_scores.std(axis=1)\n",
    "    \n",
    "    axes[i].semilogx(param_range, train_mean, 'o-', color='blue', \n",
    "                     label='Training Score')\n",
    "    axes[i].fill_between(param_range, train_mean - train_std, \n",
    "                        train_mean + train_std, alpha=0.1, color='blue')\n",
    "    \n",
    "    axes[i].semilogx(param_range, val_mean, 'o-', color='red',\n",
    "                     label='Validation Score') \n",
    "    axes[i].fill_between(param_range, val_mean - val_std,\n",
    "                        val_mean + val_std, alpha=0.1, color='red')\n",
    "    \n",
    "    # Mark best parameter\n",
    "    best_alpha = tuning_results[model_name]['best_params'][param_name]\n",
    "    axes[i].axvline(best_alpha, color='green', linestyle='--', \n",
    "                   label=f'Best Î± = {best_alpha:.3f}')\n",
    "    \n",
    "    axes[i].set_xlabel('Alpha (Regularization Strength)')\n",
    "    axes[i].set_ylabel('RÂ² Score')\n",
    "    axes[i].set_title(f'{model_name} Validation Curve')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ“Š VALIDATION CURVE INSIGHTS:\")\n",
    "print(\"- Left side (low alpha): High variance, potential overfitting\")\n",
    "print(\"- Right side (high alpha): High bias, potential underfitting\")  \n",
    "print(\"- Sweet spot: Where validation score peaks\")\n",
    "print(\"- Gap between curves indicates overfitting tendency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9831762",
   "metadata": {},
   "source": [
    "## Polynomial Features: Capturing Non-Linear Relationships ðŸŒŸ\n",
    "\n",
    "Linear models assume linear relationships, but real life is often non-linear!\n",
    "Let's add polynomial features to capture curves and interactions.\n",
    "\n",
    "**Example:** Instead of just `income`, we can include `incomeÂ²`, `incomeÂ³`, `income Ã— age`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca7cc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate polynomial features with a subset of data\n",
    "print(\"ðŸŒŸ POLYNOMIAL FEATURES EXPLORATION\")\n",
    "\n",
    "# Use a smaller subset for clear visualization\n",
    "subset_size = 1000\n",
    "indices = np.random.choice(len(X_housing), subset_size, replace=False)\n",
    "X_subset = X_housing.iloc[indices]\n",
    "y_subset = y_housing[indices]\n",
    "\n",
    "# Focus on the most important feature: MedInc (Median Income)\n",
    "X_simple = X_subset[['MedInc']].copy()\n",
    "y_simple = y_subset.copy()\n",
    "\n",
    "# Split the simple dataset\n",
    "X_train_simple, X_test_simple, y_train_simple, y_test_simple = train_test_split(\n",
    "    X_simple, y_simple, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Compare different polynomial degrees\n",
    "degrees = [1, 2, 3, 4, 5]\n",
    "poly_results = {}\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, degree in enumerate(degrees):\n",
    "    # Create polynomial features\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_train_poly = poly.fit_transform(X_train_simple)\n",
    "    X_test_poly = poly.transform(X_test_simple)\n",
    "    \n",
    "    # Fit linear regression with polynomial features\n",
    "    lr_poly = LinearRegression()\n",
    "    lr_poly.fit(X_train_poly, y_train_simple)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_train = lr_poly.predict(X_train_poly)\n",
    "    y_pred_test = lr_poly.predict(X_test_poly)\n",
    "    \n",
    "    # Metrics\n",
    "    train_r2 = r2_score(y_train_simple, y_pred_train)\n",
    "    test_r2 = r2_score(y_test_simple, y_pred_test)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test_simple, y_pred_test))\n",
    "    \n",
    "    poly_results[f'Degree {degree}'] = {\n",
    "        'Train RÂ²': train_r2,\n",
    "        'Test RÂ²': test_r2,\n",
    "        'RMSE': test_rmse,\n",
    "        'Overfitting': train_r2 - test_r2\n",
    "    }\n",
    "    \n",
    "    # Plot the polynomial fit\n",
    "    if i < len(axes):\n",
    "        # Create smooth line for visualization\n",
    "        X_plot = np.linspace(X_simple['MedInc'].min(), X_simple['MedInc'].max(), 100)\n",
    "        X_plot_poly = poly.transform(X_plot.reshape(-1, 1))\n",
    "        y_plot = lr_poly.predict(X_plot_poly)\n",
    "        \n",
    "        axes[i].scatter(X_test_simple['MedInc'], y_test_simple, alpha=0.5, s=20)\n",
    "        axes[i].plot(X_plot, y_plot, 'r-', linewidth=2)\n",
    "        axes[i].set_xlabel('Median Income')\n",
    "        axes[i].set_ylabel('House Price ($100k)')\n",
    "        axes[i].set_title(f'Polynomial Degree {degree}\\nTest RÂ² = {test_r2:.3f}')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "# Performance comparison in the last subplot\n",
    "if len(degrees) < len(axes):\n",
    "    poly_df = pd.DataFrame(poly_results).T\n",
    "    \n",
    "    axes[-1].plot(degrees, poly_df['Train RÂ²'], 'o-', label='Training RÂ²', linewidth=2)\n",
    "    axes[-1].plot(degrees, poly_df['Test RÂ²'], 'o-', label='Test RÂ²', linewidth=2)\n",
    "    axes[-1].set_xlabel('Polynomial Degree')\n",
    "    axes[-1].set_ylabel('RÂ² Score')\n",
    "    axes[-1].set_title('Polynomial Degree vs Performance')\n",
    "    axes[-1].legend()\n",
    "    axes[-1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display results table\n",
    "poly_df = pd.DataFrame(poly_results).T\n",
    "print(\"ðŸ“Š POLYNOMIAL FEATURES RESULTS:\")\n",
    "print(poly_df.round(3))\n",
    "\n",
    "# Identify optimal degree\n",
    "optimal_degree = poly_df['Test RÂ²'].idxmax()\n",
    "print(f\"\\nðŸŽ¯ OPTIMAL POLYNOMIAL DEGREE: {optimal_degree}\")\n",
    "print(f\"Achieves Test RÂ² of {poly_df.loc[optimal_degree, 'Test RÂ²']:.3f}\")\n",
    "\n",
    "if poly_df.loc[optimal_degree, 'Overfitting'] > 0.1:\n",
    "    print(\"âš ï¸  Warning: Significant overfitting detected!\")\n",
    "    print(\"Consider regularization with polynomial features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a15b3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine polynomial features with regularization\n",
    "print(\"ðŸŽšï¸ POLYNOMIAL FEATURES + REGULARIZATION\")\n",
    "\n",
    "# Use the optimal degree found above\n",
    "optimal_deg = int(optimal_degree.split()[1])\n",
    "\n",
    "# Create pipeline with polynomial features and Ridge regression\n",
    "poly_ridge_pipeline = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=optimal_deg, include_bias=False)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('ridge', Ridge())\n",
    "])\n",
    "\n",
    "# Grid search for optimal regularization\n",
    "param_grid_poly = {\n",
    "    'ridge__alpha': np.logspace(-2, 2, 20)\n",
    "}\n",
    "\n",
    "print(\"Searching for optimal regularization with polynomial features...\")\n",
    "grid_search_poly = GridSearchCV(\n",
    "    poly_ridge_pipeline, param_grid_poly, cv=5, scoring='r2', n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search_poly.fit(X_train_simple, y_train_simple)\n",
    "\n",
    "print(f\"Best alpha for polynomial Ridge: {grid_search_poly.best_params_['ridge__alpha']:.4f}\")\n",
    "print(f\"Best CV RÂ² score: {grid_search_poly.best_score_:.3f}\")\n",
    "\n",
    "# Test performance\n",
    "y_pred_poly_ridge = grid_search_poly.predict(X_test_simple)\n",
    "test_r2_poly_ridge = r2_score(y_test_simple, y_pred_poly_ridge)\n",
    "test_rmse_poly_ridge = np.sqrt(mean_squared_error(y_test_simple, y_pred_poly_ridge))\n",
    "\n",
    "print(f\"\\nðŸ† POLYNOMIAL + RIDGE RESULTS:\")\n",
    "print(f\"Test RÂ² Score: {test_r2_poly_ridge:.3f}\")\n",
    "print(f\"Test RMSE: ${test_rmse_poly_ridge:.1f}k\")\n",
    "print(f\"Improvement over linear: {test_r2_poly_ridge - poly_df.loc['Degree 1', 'Test RÂ²']:.3f}\")\n",
    "\n",
    "# Compare all approaches on the simple dataset\n",
    "comparison_simple = {\n",
    "    'Linear': poly_df.loc['Degree 1', 'Test RÂ²'],\n",
    "    f'Polynomial (deg {optimal_deg})': poly_df.loc[optimal_degree, 'Test RÂ²'],\n",
    "    f'Polynomial + Ridge': test_r2_poly_ridge\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(comparison_simple.keys(), comparison_simple.values(), \n",
    "               alpha=0.7, color=['blue', 'orange', 'green'])\n",
    "plt.ylabel('Test RÂ² Score')\n",
    "plt.title('Model Comparison: Linear vs Polynomial vs Regularized Polynomial')\n",
    "plt.ylim(0, max(comparison_simple.values()) * 1.1)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, comparison_simple.values()):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "             f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6d4a26",
   "metadata": {},
   "source": [
    "### ðŸŽ® Interactive Exercise 3: Complete Regression Pipeline\n",
    "\n",
    "**Final Challenge:** Build a production-ready regression pipeline for the full housing dataset!\n",
    "\n",
    "**Your Mission:**\n",
    "1. Use all features with polynomial interactions\n",
    "2. Include feature selection to avoid overfitting\n",
    "3. Apply proper cross-validation\n",
    "4. Compare multiple algorithms (Linear, Ridge, Random Forest)\n",
    "5. Create learning curves to assess if more data would help\n",
    "\n",
    "**Real-World Skill:** This is how you'd approach a regression problem in practice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec09184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build complete production pipeline\n",
    "\n",
    "print(\"ðŸ­ BUILDING PRODUCTION-READY REGRESSION PIPELINE\")\n",
    "\n",
    "# Define comprehensive pipeline components\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Multiple pipeline configurations to compare\n",
    "pipelines = {\n",
    "    'Linear + PolyFeatures': Pipeline([\n",
    "        ('poly', PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('feature_select', SelectKBest(f_regression, k=20)),  # Select top 20 features\n",
    "        ('regressor', LinearRegression())\n",
    "    ]),\n",
    "    \n",
    "    'Ridge + PolyFeatures': Pipeline([\n",
    "        ('poly', PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('feature_select', SelectKBest(f_regression, k=20)),\n",
    "        ('regressor', Ridge(alpha=1.0))\n",
    "    ]),\n",
    "    \n",
    "    'Lasso + PolyFeatures': Pipeline([\n",
    "        ('poly', PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', Lasso(alpha=0.1, max_iter=2000))  # Lasso does its own feature selection\n",
    "    ]),\n",
    "    \n",
    "    'Random Forest': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "    ]),\n",
    "    \n",
    "    'Optimized Ridge': Pipeline([\n",
    "        ('poly', PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('feature_select', SelectKBest(f_regression, k=15)),\n",
    "        ('regressor', Ridge())\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Parameter grids for optimization\n",
    "param_grids = {\n",
    "    'Linear + PolyFeatures': {},  # No hyperparameters to tune\n",
    "    \n",
    "    'Ridge + PolyFeatures': {\n",
    "        'regressor__alpha': [0.1, 1.0, 10.0]\n",
    "    },\n",
    "    \n",
    "    'Lasso + PolyFeatures': {\n",
    "        'regressor__alpha': [0.01, 0.1, 1.0]\n",
    "    },\n",
    "    \n",
    "    'Random Forest': {\n",
    "        'regressor__n_estimators': [50, 100, 200],\n",
    "        'regressor__max_depth': [10, 20, None]\n",
    "    },\n",
    "    \n",
    "    'Optimized Ridge': {\n",
    "        'feature_select__k': [10, 15, 20],\n",
    "        'regressor__alpha': [0.1, 1.0, 10.0, 100.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Cross-validate all pipelines\n",
    "pipeline_results = {}\n",
    "\n",
    "print(\"Training and evaluating pipelines...\")\n",
    "for name, pipeline in pipelines.items():\n",
    "    print(f\"\\nProcessing {name}...\")\n",
    "    \n",
    "    if param_grids[name]:  # Has hyperparameters to tune\n",
    "        grid_search = GridSearchCV(\n",
    "            pipeline, param_grids[name], cv=5, scoring='r2', n_jobs=-1\n",
    "        )\n",
    "        grid_search.fit(X_train_housing, y_train_housing)\n",
    "        \n",
    "        best_pipeline = grid_search.best_estimator_\n",
    "        cv_score = grid_search.best_score_\n",
    "        best_params = grid_search.best_params_\n",
    "        \n",
    "        print(f\"Best parameters: {best_params}\")\n",
    "        \n",
    "    else:  # No hyperparameters to tune\n",
    "        cv_scores = cross_val_score(pipeline, X_train_housing, y_train_housing, \n",
    "                                   cv=5, scoring='r2', n_jobs=-1)\n",
    "        cv_score = cv_scores.mean()\n",
    "        best_pipeline = pipeline\n",
    "        best_pipeline.fit(X_train_housing, y_train_housing)\n",
    "        best_params = \"No tuning\"\n",
    "    \n",
    "    # Test set evaluation\n",
    "    y_pred_pipeline = best_pipeline.predict(X_test_housing)\n",
    "    test_r2 = r2_score(y_test_housing, y_pred_pipeline)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test_housing, y_pred_pipeline))\n",
    "    test_mae = mean_absolute_error(y_test_housing, y_pred_pipeline)\n",
    "    \n",
    "    pipeline_results[name] = {\n",
    "        'CV_R2': cv_score,\n",
    "        'Test_R2': test_r2,\n",
    "        'Test_RMSE': test_rmse,\n",
    "        'Test_MAE': test_mae,\n",
    "        'Generalization_Gap': cv_score - test_r2,\n",
    "        'Best_Pipeline': best_pipeline\n",
    "    }\n",
    "    \n",
    "    print(f\"CV RÂ²: {cv_score:.3f}\")\n",
    "    print(f\"Test RÂ²: {test_r2:.3f}\")\n",
    "    print(f\"Test RMSE: ${test_rmse:.1f}k\")\n",
    "\n",
    "# Create comprehensive results DataFrame\n",
    "pipeline_df = pd.DataFrame(pipeline_results).T.drop('Best_Pipeline', axis=1)\n",
    "print(\"\\nðŸ† FINAL PIPELINE COMPARISON:\")\n",
    "print(pipeline_df.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaafe493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize pipeline comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# RÂ² Scores\n",
    "axes[0,0].bar(range(len(pipeline_df)), pipeline_df['Test_R2'], alpha=0.7, color='skyblue')\n",
    "axes[0,0].set_xticks(range(len(pipeline_df)))\n",
    "axes[0,0].set_xticklabels(pipeline_df.index, rotation=45, ha='right')\n",
    "axes[0,0].set_ylabel('Test RÂ² Score')\n",
    "axes[0,0].set_title('Model RÂ² Performance')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# RMSE\n",
    "axes[0,1].bar(range(len(pipeline_df)), pipeline_df['Test_RMSE'], alpha=0.7, color='lightcoral')\n",
    "axes[0,1].set_xticks(range(len(pipeline_df)))\n",
    "axes[0,1].set_xticklabels(pipeline_df.index, rotation=45, ha='right')\n",
    "axes[0,1].set_ylabel('Test RMSE ($100k)')\n",
    "axes[0,1].set_title('Model RMSE (Lower is Better)')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Generalization Gap\n",
    "axes[1,0].bar(range(len(pipeline_df)), pipeline_df['Generalization_Gap'], alpha=0.7, color='gold')\n",
    "axes[1,0].set_xticks(range(len(pipeline_df)))\n",
    "axes[1,0].set_xticklabels(pipeline_df.index, rotation=45, ha='right')\n",
    "axes[1,0].set_ylabel('CV RÂ² - Test RÂ²')\n",
    "axes[1,0].set_title('Generalization Gap (Lower is Better)')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# CV vs Test RÂ²\n",
    "axes[1,1].scatter(pipeline_df['CV_R2'], pipeline_df['Test_R2'], s=100, alpha=0.7)\n",
    "axes[1,1].plot([0, 1], [0, 1], 'r--', alpha=0.5)  # Perfect generalization line\n",
    "for i, model in enumerate(pipeline_df.index):\n",
    "    axes[1,1].annotate(model, (pipeline_df.iloc[i]['CV_R2'], pipeline_df.iloc[i]['Test_R2']),\n",
    "                      xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "axes[1,1].set_xlabel('Cross-Validation RÂ²')\n",
    "axes[1,1].set_ylabel('Test RÂ²')\n",
    "axes[1,1].set_title('Cross-Validation vs Test Performance')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify champion\n",
    "best_model_name = pipeline_df['Test_R2'].idxmax()\n",
    "best_model = pipeline_results[best_model_name]['Best_Pipeline']\n",
    "\n",
    "print(f\"\\nðŸ¥‡ CHAMPION MODEL: {best_model_name}\")\n",
    "print(f\"Test RÂ² Score: {pipeline_df.loc[best_model_name, 'Test_R2']:.3f}\")\n",
    "print(f\"Test RMSE: ${pipeline_df.loc[best_model_name, 'Test_RMSE']:.1f}k\")\n",
    "print(f\"Generalization Gap: {pipeline_df.loc[best_model_name, 'Generalization_Gap']:.3f}\")\n",
    "\n",
    "# Business interpretation\n",
    "best_rmse = pipeline_df.loc[best_model_name, 'Test_RMSE']\n",
    "best_r2 = pipeline_df.loc[best_model_name, 'Test_R2']\n",
    "\n",
    "print(f\"\\nðŸ’° BUSINESS IMPACT:\")\n",
    "print(f\"- Model explains {best_r2:.1%} of house price variation\")\n",
    "print(f\"- Typical prediction error: ${best_rmse:.1f}k\")\n",
    "print(f\"- Suitable for: Property valuation, market analysis, investment decisions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c52f4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning curves for the best model\n",
    "print(f\"ðŸ“ˆ LEARNING CURVES FOR {best_model_name}\")\n",
    "\n",
    "train_sizes, train_scores_lc, val_scores_lc = learning_curve(\n",
    "    best_model, X_train_housing, y_train_housing, \n",
    "    cv=5, train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "    scoring='r2', n_jobs=-1\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Learning curve\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(train_sizes, train_scores_lc.mean(axis=1), 'o-', label='Training Score')\n",
    "plt.plot(train_sizes, val_scores_lc.mean(axis=1), 'o-', label='Validation Score')\n",
    "plt.fill_between(train_sizes, \n",
    "                 train_scores_lc.mean(axis=1) - train_scores_lc.std(axis=1),\n",
    "                 train_scores_lc.mean(axis=1) + train_scores_lc.std(axis=1), alpha=0.1)\n",
    "plt.fill_between(train_sizes,\n",
    "                 val_scores_lc.mean(axis=1) - val_scores_lc.std(axis=1),\n",
    "                 val_scores_lc.mean(axis=1) + val_scores_lc.std(axis=1), alpha=0.1)\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('RÂ² Score')\n",
    "plt.title('Learning Curves')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Prediction vs Actual for best model\n",
    "y_pred_best = best_model.predict(X_test_housing)\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.scatter(y_test_housing, y_pred_best, alpha=0.6, s=20)\n",
    "plt.plot([y_test_housing.min(), y_test_housing.max()], \n",
    "         [y_test_housing.min(), y_test_housing.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Price ($100k)')\n",
    "plt.ylabel('Predicted Price ($100k)')\n",
    "plt.title(f'{best_model_name}: Predictions vs Actual')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Residual plot\n",
    "residuals_best = y_test_housing - y_pred_best\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.scatter(y_pred_best, residuals_best, alpha=0.6, s=20)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted Price ($100k)')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Residual histogram\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.hist(residuals_best, bins=30, alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Residuals Distribution')\n",
    "plt.axvline(x=0, color='r', linestyle='--')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Learning curve interpretation\n",
    "final_gap = train_scores_lc.mean(axis=1)[-1] - val_scores_lc.mean(axis=1)[-1]\n",
    "final_val_score = val_scores_lc.mean(axis=1)[-1]\n",
    "\n",
    "print(f\"\\nðŸ“Š LEARNING CURVE ANALYSIS:\")\n",
    "print(f\"Final validation score: {final_val_score:.3f}\")\n",
    "print(f\"Training-validation gap: {final_gap:.3f}\")\n",
    "\n",
    "if final_gap > 0.05:\n",
    "    print(\"ðŸ”´ Still overfitting - consider:\")\n",
    "    print(\"  â€¢ More regularization\")\n",
    "    print(\"  â€¢ More training data\") \n",
    "    print(\"  â€¢ Feature selection\")\n",
    "elif final_val_score < 0.7:\n",
    "    print(\"ðŸŸ¡ Underfitting - consider:\")\n",
    "    print(\"  â€¢ More complex model\")\n",
    "    print(\"  â€¢ More features\")\n",
    "    print(\"  â€¢ Less regularization\")\n",
    "else:\n",
    "    print(\"ðŸŸ¢ Good bias-variance tradeoff!\")\n",
    "\n",
    "# Check if more data would help\n",
    "if val_scores_lc.mean(axis=1)[-1] > val_scores_lc.mean(axis=1)[-2]:\n",
    "    print(\"ðŸ“ˆ Performance still improving - more data might help!\")\n",
    "else:\n",
    "    print(\"ðŸ“‰ Performance plateaued - more data may not help much\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e63d514",
   "metadata": {},
   "source": [
    "## ðŸ† Workshop Summary & Key Insights\n",
    "\n",
    "Congratulations! You've mastered comprehensive regression analysis. Here's what you've accomplished:\n",
    "\n",
    "### ðŸŽ¯ Technical Skills Mastered:\n",
    "\n",
    "1. **Linear Regression Fundamentals**: Built and evaluated basic models\n",
    "2. **Regularization Techniques**: Applied Ridge, Lasso, and Elastic Net\n",
    "3. **Polynomial Features**: Captured non-linear relationships\n",
    "4. **Pipeline Construction**: Built production-ready ML pipelines\n",
    "5. **Hyperparameter Tuning**: Optimized models with GridSearchCV\n",
    "6. **Feature Selection**: Automated feature selection techniques\n",
    "7. **Model Evaluation**: Comprehensive metrics beyond just RÂ²\n",
    "8. **Learning Curves**: Diagnosed bias vs variance tradeoffs\n",
    "\n",
    "### ðŸŽ¨ Advanced Techniques Applied:\n",
    "\n",
    "- **Feature Engineering**: Polynomial and interaction terms\n",
    "- **Cross-Validation**: Robust model selection\n",
    "- **Regularization Paths**: Understanding coefficient behavior\n",
    "- **Pipeline Integration**: Seamless preprocessing and modeling\n",
    "- **Ensemble Methods**: Random Forest for non-linear patterns\n",
    "\n",
    "### ðŸ’° Business Impact Understanding:\n",
    "\n",
    "- **Model Selection**: Choosing algorithms based on problem requirements\n",
    "- **Performance Metrics**: RMSE for interpretable error magnitude\n",
    "- **Overfitting Detection**: Recognizing and preventing poor generalization\n",
    "- **Production Readiness**: Building models that work in the real world\n",
    "\n",
    "### ðŸš€ Next Level Recommendations:\n",
    "\n",
    "1. **Advanced Ensemble Methods**: XGBoost, LightGBM\n",
    "2. **Time Series Regression**: When data has temporal components\n",
    "3. **Bayesian Regression**: Uncertainty quantification\n",
    "4. **Deep Learning**: Neural networks for complex patterns\n",
    "5. **Model Deployment**: Flask, FastAPI, cloud platforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e9e695",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸŽ‰ REGRESSION WORKSHOP COMPLETE! ðŸŽ‰\")\n",
    "print(\"\\nðŸ“ˆ Regression Mastery Achieved:\")\n",
    "print(\"âœ… Linear and regularized regression\")\n",
    "print(\"âœ… Polynomial feature engineering\")\n",
    "print(\"âœ… Advanced model pipelines\")\n",
    "print(\"âœ… Hyperparameter optimization\")\n",
    "print(\"âœ… Production-ready evaluation\")\n",
    "\n",
    "print(f\"\\nðŸ† Your Champion Model: {best_model_name}\")\n",
    "print(f\"ðŸ“Š Performance: RÂ² = {pipeline_df.loc[best_model_name, 'Test_R2']:.3f}\")\n",
    "print(f\"ðŸ’° Business Value: ${pipeline_df.loc[best_model_name, 'Test_RMSE']:.1f}k average error\")\n",
    "\n",
    "print(\"\\nðŸš€ You're now ready to:\")\n",
    "print(\"â€¢ Predict house prices, sales figures, stock prices\")\n",
    "print(\"â€¢ Build recommendation systems with ratings\")\n",
    "print(\"â€¢ Forecast demand, revenue, and other business metrics\")\n",
    "print(\"â€¢ Handle any continuous prediction problem!\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ FINAL CHALLENGE:\")\n",
    "print(\"Can you build a model that achieves RÂ² > 0.85 on the housing dataset?\")\n",
    "print(\"Hint: Try feature engineering, ensemble methods, or stacking!\")\n",
    "\n",
    "# Save the best model for future use (in practice)\n",
    "print(f\"\\nðŸ’¾ Best model pipeline saved conceptually:\")\n",
    "print(\"In production, you'd use joblib.dump() to save this pipeline!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
